import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import xgboost as xgb 
import matplotlib
import plotly.express as px

# Configuration de la page
st.set_page_config(
    page_title="Analyse des proc√©d√©s Sanofi",
    page_icon="üíä",
    layout="wide"
)

# Titre et description
st.title("Analyse des Proc√©d√©s de Production Sanofi")
st.markdown("""
Cette application permet de visualiser et analyser les donn√©es de capteurs 
pour les lots de production pharmaceutique. Elle aide √† identifier 
les d√©viations de proc√©d√©s en √©tudiant l'√©volution de la temp√©rature et d'autres param√®tres.
""")

# Fonction pour charger les donn√©es
@st.cache_data
def load_data(file_path):
    try:
        data = pd.read_csv(file_path)
        # Conversion de la colonne Time en datetime si n√©cessaire
        if 'Time' in data.columns:
            data['Time'] = pd.to_datetime(data['Time'])
        return data
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es: {e}")
        return None

# Sidebar pour le chargement des donn√©es et les options
with st.sidebar:
    st.header("Configuration")
    
    # Upload des donn√©es
    uploaded_file = st.file_uploader("Charger le fichier CSV des donn√©es", type=['csv'])
    
    if uploaded_file is not None:
        data = load_data(uploaded_file)
    else:
        st.info("Veuillez charger un fichier CSV pour commencer l'analyse.")

# Corps principal de l'application
if 'data' in locals() and data is not None:
    # V√©rification des valeurs manquantes
    missing_values = data.isnull().sum().sum()  # Total des valeurs manquantes
    if missing_values > 0:
        with st.expander(f"‚ö†Ô∏è {missing_values} valeur(s) manquante(s) d√©tect√©e(s)"):
            # Afficher les lignes o√π il manque des valeurs
            missing_data = data[data.isnull().any(axis=1)]
            st.write(f"Voici les lignes avec des valeurs manquantes :")
            st.dataframe(missing_data)
    else:
        st.success("Aucune valeur manquante d√©tect√©e.")
    st.header("Exploration des Donn√©es")
    
    # Statistiques de base
    st.subheader("R√©sum√© des Donn√©es")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"Nombre total d'observations: {data.shape[0]}")
        st.write(f"Nombre de lots: {data['Batch name'].nunique()}")
    with col2:
        st.write(f"√âtapes du proc√©d√©: {', '.join(data['Step'].unique())}")
        
    # Aper√ßu des donn√©es
    if st.checkbox("Afficher l'aper√ßu des donn√©es"):
        st.dataframe(data.head())

    # R√©sum√© statistique des donn√©es
    st.header("R√©sum√© statistique des donn√©es")
    st.subheader("üìä Statistiques Descriptives Globales")
    st.markdown("""
        Cette section permet d'analyser les statistiques globaux de l'ensemble de la base.
        """)
    stats_globales = data.describe().T # Transpose pour un affichage plus lisible
    stats_globales = stats_globales.drop(index='Time', errors='ignore')  # Suppression de la ligne 'Time'
    st.dataframe(stats_globales)
    
    # Section de s√©lection des lots
    st.header("Visualisation des Lots")
    
    # Onglets pour les diff√©rentes visualisations
    tabs = st.tabs(["Visualisation individuelle", "Superposition (Batch Overlay)", "Analyse comparative"])
    
    with tabs[0]:
        st.subheader("Visualisation d'un Lot Individuel")
        
        # S√©lection du lot et de l'√©tape
        col1, col2 = st.columns(2)
        with col1:
            selected_batch = st.selectbox("S√©lectionner un lot", options=sorted(data['Batch name'].unique()), key="vis_batch")
        with col2:
            selected_step = st.selectbox("S√©lectionner une √©tape (optionnel)", 
                                       options=["Toutes les √©tapes"] + sorted(data['Step'].unique()), key="vis_step")
        
        # Filtrage des donn√©es
        if selected_step == "Toutes les √©tapes":
            filtered_data = data[data['Batch name'] == selected_batch]
        else:
            filtered_data = data[(data['Batch name'] == selected_batch) & (data['Step'] == selected_step)]
            
        if not filtered_data.empty:
            # S√©lection des param√®tres √† visualiser
            params = st.multiselect(
                "S√©lectionner les param√®tres √† visualiser",
                options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
                default=['Temp√©rature fond de cuve', 'Temp√©rature haut de colonne', 'Temp√©rature r√©acteur'],
                key="vis_params"
            )
            
            if params:
                # Cr√©ation du graphique avec Streamlit native
                if 'Time' in filtered_data.columns:
                    chart_data = filtered_data.set_index('Time')[params]
                else:
                    chart_data = filtered_data[params]
                
                st.line_chart(chart_data)
                
                # Option pour t√©l√©charger les donn√©es filtr√©es
                csv = filtered_data.to_csv(index=False)
                st.download_button(
                    label="T√©l√©charger les donn√©es filtr√©es",
                    data=csv,
                    file_name=f"{selected_batch}_{selected_step.replace(' ', '_')}.csv",
                    mime='text/csv',
                )
            else:
                st.warning("Veuillez s√©lectionner au moins un param√®tre √† visualiser.")
        else:
            st.warning("Aucune donn√©e disponible pour ce lot et cette √©tape.")
    
    with tabs[1]:
        st.subheader("Superposition des Lots (Batch Overlay)")
        
        # S√©lection des lots pour la superposition
        col1, col2 = st.columns(2)
        with col1:
            selected_batches = st.multiselect(
                "S√©lectionner les lots √† superposer",
                options=sorted(data['Batch name'].unique()),
                default=sorted(data['Batch name'].unique())[:2] if len(data['Batch name'].unique()) >= 2 else [],
                key="overlay_batches"
            )
        with col2:
            overlay_step = st.selectbox("√âtape pour la superposition", 
                                     options=sorted(data['Step'].unique()),
                                     key="overlay_step")
        
        if selected_batches and overlay_step:
            # S√©lection du param√®tre √† visualiser
            overlay_param = st.selectbox(
                "Param√®tre √† superposer",
                options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
                index=data.columns.get_loc("Temp√©rature fond de cuve") - 2 if "Temp√©rature fond de cuve" in data.columns else 0,
                key="overlay_param"
            )
            
            # Pr√©paration des donn√©es pour le graphique
            overlay_data = pd.DataFrame()
            
            # Pr√©parer les donn√©es pour chaque lot
            for batch in selected_batches:
                batch_data = data[(data['Batch name'] == batch) & (data['Step'] == overlay_step)]
                # Cr√©er un nouveau DataFrame √† chaque it√©ration au lieu d'ajouter √† un existant
                batch_series = pd.Series(batch_data[overlay_param].values)
                if overlay_data.empty:
                    overlay_data = pd.DataFrame({batch: batch_series})
                else:
                    # R√©indexer √† la m√™me longueur si n√©cessaire
                    max_len = max(len(overlay_data), len(batch_series))
                    # √âtendre l'overlay_data existant si n√©cessaire
                    if len(overlay_data) < max_len:
                        overlay_data = overlay_data.reindex(range(max_len), fill_value=np.nan)
                    # √âtendre la nouvelle s√©rie si n√©cessaire
                    if len(batch_series) < max_len:
                        batch_series = batch_series.reindex(range(max_len), fill_value=np.nan)
                    # Ajouter la nouvelle s√©rie
                    overlay_data[batch] = batch_series
            
            # Afficher le graphique
            if not overlay_data.empty:
                # Utiliser le graphique natif de Streamlit
                st.line_chart(overlay_data)
                
                # Option simple pour ajuster les donn√©es
                st.subheader("Filtrage des donn√©es")
                filter_start, filter_end = st.slider(
                    "Filtrer les donn√©es (% de progression)",
                    0, 100, (0, 100),
                    step=5,
                    key="filter_slider"
                )
                
                if filter_start > 0 or filter_end < 100:
                    st.info(f"Filtrage appliqu√©: {filter_start}% √† {filter_end}% de la progression")
                    
                    # Filtrer les donn√©es pour chaque lot
                    filtered_overlay_data = pd.DataFrame()
                    filtered_data_dict = {}
                    
                    for batch in selected_batches:
                        batch_data = data[(data['Batch name'] == batch) & (data['Step'] == overlay_step)]
                        if not batch_data.empty:
                            batch_data = batch_data.reset_index(drop=True)
                            x_norm = np.linspace(0, 100, len(batch_data))
                            
                            # Appliquer le filtre
                            mask = (x_norm >= filter_start) & (x_norm <= filter_end)
                            filtered_overlay_data[batch] = batch_data[overlay_param].iloc[mask].reset_index(drop=True)
                            filtered_data_dict[batch] = batch_data[overlay_param].iloc[mask].reset_index(drop=True)
                    
                    # Afficher le graphique filtr√©
                    if not filtered_overlay_data.empty:
                        st.line_chart(filtered_overlay_data)
                        
                        # Pr√©paration pour t√©l√©chargement
                        if filtered_data_dict:
                            # S'assurer que toutes les s√©ries ont la m√™me longueur
                            max_len = max([len(series) for series in filtered_data_dict.values()])
                            for batch, series in filtered_data_dict.items():
                                if len(series) < max_len:
                                    # Padding avec NaN
                                    filtered_data_dict[batch] = pd.Series(list(series) + [np.nan] * (max_len - len(series)))
                            
                            filtered_df = pd.DataFrame(filtered_data_dict)
                            
                            # Option de t√©l√©chargement
                            csv = filtered_df.to_csv(index=True)
                            st.download_button(
                                label="T√©l√©charger les donn√©es filtr√©es",
                                data=csv,
                                file_name=f"filtered_overlay_{overlay_param}_{overlay_step}.csv",
                                mime='text/csv',
                            )
            else:
                st.warning("Pas de donn√©es disponibles pour la superposition.")
        else:
            st.warning("Veuillez s√©lectionner au moins un lot et une √©tape pour la superposition.")
    
    with tabs[2]:
        st.subheader("Analyse Comparative et D√©tection des D√©viations")
        
        # S√©lectionner un lot de r√©f√©rence (id√©al) et un lot √† comparer
        col1, col2 = st.columns(2)
        with col1:
            ideal_batch = st.selectbox("Lot de r√©f√©rence (id√©al)", 
                                     options=sorted(data['Batch name'].unique()),
                                     key="ideal_batch")
        with col2:
            compare_batch = st.selectbox("Lot √† comparer", 
                                      options=[b for b in sorted(data['Batch name'].unique()) if b != ideal_batch],
                                      key="compare_batch")
        
        # S√©lectionner l'√©tape √† comparer
        compare_step = st.selectbox("√âtape √† comparer", 
                                  options=sorted(data['Step'].unique()),
                                  key="compare_step")
        
        if ideal_batch and compare_batch and compare_step:
            # Filtrer les donn√©es
            ideal_data = data[(data['Batch name'] == ideal_batch) & (data['Step'] == compare_step)]
            compare_data = data[(data['Batch name'] == compare_batch) & (data['Step'] == compare_step)]
            
            if not ideal_data.empty and not compare_data.empty:
                # S√©lectionner les param√®tres √† comparer
                compare_params = st.multiselect(
                    "Param√®tres √† comparer",
                    options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
                    default=['Temp√©rature fond de cuve', 'Temp√©rature haut de colonne'],
                    key="compare_params"
                )
                
                if compare_params:
                    # Cr√©er une analyse pour chaque param√®tre
                    for param in compare_params:
                        # Pr√©parer les donn√©es pour l'analyse
                        ideal_data_reset = ideal_data.reset_index(drop=True)
                        compare_data_reset = compare_data.reset_index(drop=True)
                        
                        # Assurer que les deux s√©ries ont la m√™me longueur
                        min_len = min(len(ideal_data_reset), len(compare_data_reset))
                        ideal_series = ideal_data_reset[param].iloc[:min_len]
                        compare_series = compare_data_reset[param].iloc[:min_len]
                        
                        # Calculer la diff√©rence absolue
                        diff = abs(ideal_series.values - compare_series.values)
                        
                        # Cr√©er un DataFrame pour le graphique
                        comparison_df = pd.DataFrame({
                            f"{ideal_batch} (R√©f√©rence)": ideal_series.values,
                            f"{compare_batch}": compare_series.values,
                            "Diff√©rence absolue": diff
                        })
                        
                        # Afficher le graphique de comparaison
                        st.subheader(f"Comparaison de {param} - {compare_step}")
                        st.line_chart(comparison_df)
                        
                        # Seuil de d√©viation
                        threshold = st.slider(f"Seuil de d√©viation pour {param}", 
                                           0.0, float(max(diff)*1.5), float(max(diff)*0.2),
                                           key=f"threshold_{param}")
                        
                        # Identifier les zones de d√©viation
                        deviation_indices = np.where(diff > threshold)[0]
                        
                        if len(deviation_indices) > 0:
                            # Grouper les indices cons√©cutifs pour trouver les zones
                            ranges = []
                            if len(deviation_indices) > 0:
                                start = deviation_indices[0]
                                for i in range(1, len(deviation_indices)):
                                    if deviation_indices[i] != deviation_indices[i-1] + 1:
                                        ranges.append((start, deviation_indices[i-1]))
                                        start = deviation_indices[i]
                                ranges.append((start, deviation_indices[-1]))
                            
                            # Cr√©er un DataFrame pour visualiser les d√©viations
                            deviation_df = pd.DataFrame({
                                f"{ideal_batch} (R√©f√©rence)": ideal_series.values,
                                f"{compare_batch}": compare_series.values,
                                "Diff√©rence absolue": diff,
                                "Seuil": [threshold] * len(diff)
                            })
                            
                            # Afficher le graphique avec le seuil
                            st.subheader(f"D√©viations d√©tect√©es pour {param}")
                            st.line_chart(deviation_df)
                            
                            # R√©sum√© des d√©viations
                            st.warning(f"D√©viations d√©tect√©es pour {param}: {len(deviation_indices)} points d√©passent le seuil.")
                            
                            # Analyse statistique des d√©viations
                            st.subheader(f"Analyse statistique des d√©viations pour {param}")
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("D√©viation maximale", f"{max(diff):.2f}")
                            with col2:
                                st.metric("D√©viation moyenne", f"{np.mean(diff):.2f}")
                            with col3:
                                st.metric("% de points en d√©viation", f"{len(deviation_indices)/min_len*100:.1f}%")
                            
                            # Afficher les points de d√©viation
                            deviation_points = pd.DataFrame({
                                "Index": deviation_indices,
                                f"{ideal_batch} (R√©f√©rence)": ideal_series.iloc[deviation_indices].values,
                                f"{compare_batch}": compare_series.iloc[deviation_indices].values,
                                "Diff√©rence": diff[deviation_indices]
                            })
                            
                            st.subheader("Points de d√©viation significative")
                            st.dataframe(deviation_points)
                        else:
                            st.success(f"Aucune d√©viation significative d√©tect√©e pour {param}.")
                else:
                    st.warning("Veuillez s√©lectionner au moins un param√®tre √† comparer.")
            else:
                st.warning("Donn√©es insuffisantes pour l'un des lots s√©lectionn√©s.")
        else:
            st.info("Veuillez s√©lectionner un lot de r√©f√©rence, un lot √† comparer et une √©tape.")
    
    # Section de mod√©lisation simplifi√©e
    st.header("Analyse Statistique")
    
    analysis_tabs = st.tabs(["Analyse des tendances", "Corr√©lations"])
    
    with analysis_tabs[1]:
        st.subheader("Analyse des Corr√©lations")
        st.markdown("""
        Cette section permet d'analyser les corr√©lations entre les diff√©rents param√®tres
        et d'identifier les relations importantes.
        """)
        
        # S√©lection des param√®tres pour l'analyse de corr√©lation
        corr_params = st.multiselect(
            "Param√®tres pour l'analyse de corr√©lation",
            options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
            default=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
            key="corr_params"
        )
        
        if corr_params:
            # Filtrer les donn√©es
            corr_data = data[corr_params].dropna()
            
            # Calculer la matrice de corr√©lation
            corr_matrix = corr_data.corr()
            
            # Afficher la matrice de corr√©lation comme un tableau
            st.subheader("Matrice de Corr√©lation entre les Param√®tres")
            st.dataframe(corr_matrix)
            
            # Analyse des relations entre variables
            st.subheader("Relations entre Variables")
            
            col1, col2 = st.columns(2)
            with col1:
                x_var = st.selectbox("Variable X", options=corr_params, index=0, key="x_var")
            with col2:
                y_var = st.selectbox("Variable Y", options=[p for p in corr_params if p != x_var], index=0, key="y_var")
            
            # Pr√©parer les donn√©es pour le scatter plot
            scatter_data = data[[x_var, y_var]].dropna()
            
            # Afficher le scatter plot
            st.subheader(f"Relation entre {x_var} et {y_var}")
            
            # Cr√©er un dataframe temporaire pour le scatter plot
            scatter_df = pd.DataFrame({
                x_var: scatter_data[x_var],
                y_var: scatter_data[y_var]
            })
            
            # Utiliser st.scatter_chart qui est disponible dans les versions r√©centes de Streamlit
            # Si ce n'est pas disponible, fallback vers une alternative
            try:
                st.scatter_chart(scatter_df, x=x_var, y=y_var)
            except:
                st.write("Nuage de points:")
                st.dataframe(scatter_df.head(100))
                st.info("Aper√ßu limit√© aux 100 premiers points. Pour une visualisation compl√®te, t√©l√©chargez les donn√©es.")
            
            # Option d'analyse par groupe simplifi√©e
            color_var = st.selectbox(
                "Grouper par",
                options=["Aucun groupement"] + ["Batch name", "Step"],
                index=0,
                key="color_var"
            )
            
            if color_var != "Aucun groupement":
                # Calculer des statistiques par groupe
                grouped_stats = data.groupby(color_var)[[x_var, y_var]].agg(['mean', 'std']).reset_index()
                
                st.subheader(f"Statistiques par {color_var}")
                st.dataframe(grouped_stats)
                
                # Afficher les statistiques sous forme de tableau
                summary_stats = data.groupby(color_var)[y_var].agg(['count', 'mean', 'std', 'min', 'max']).reset_index()
                
                st.subheader(f"Distribution de {y_var} par {color_var}")
                st.dataframe(summary_stats)
        else:
            st.warning("Veuillez s√©lectionner au moins un param√®tre pour l'analyse.")
    
    with analysis_tabs[0]:
        st.subheader("Analyse des Tendances")
        st.markdown("""
        Cette section permet d'analyser les tendances des param√®tres au fil du temps
        et d'identifier les comportements anormaux.
        """)
        
        # S√©lection du param√®tre √† analyser
        trend_param = st.selectbox(
            "Param√®tre √† analyser",
            options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
            index=0,
            key="trend_param"
        )
        
        # S√©lection de l'√©tape
        trend_step = st.selectbox(
            "√âtape √† analyser",
            options=["Toutes les √©tapes"] + sorted(data['Step'].unique()),
            key="trend_step"
        )
        
        if trend_param:
            # Filtrer les donn√©es
            if trend_step == "Toutes les √©tapes":
                trend_data = data
            else:
                trend_data = data[data['Step'] == trend_step]
            
            # Calculer les statistiques par lot
            batch_stats = trend_data.groupby('Batch name')[trend_param].agg(['mean', 'std', 'min', 'max']).reset_index()
            
            # Afficher les statistiques
            st.subheader("Statistiques par Lot")
            st.dataframe(batch_stats)
            
            # Visualiser la distribution des moyennes en utilisant st.bar_chart
            stats_for_chart = batch_stats.set_index('Batch name')[['mean']]
            st.subheader(f"Moyenne de {trend_param} par lot" + (f" - {trend_step}" if trend_step != "Toutes les √©tapes" else ""))
            st.bar_chart(stats_for_chart)
            
            # Calculer et afficher la tendance globale
            if 'Time' in trend_data.columns:
                # R√©organiser les donn√©es pour la tendance temporelle
                time_trend = trend_data.sort_values('Time')
                
                # Pr√©parer les donn√©es pour le graphique
                trend_chart_data = pd.DataFrame({
                    trend_param: time_trend[trend_param].values
                }, index=time_trend['Time'])
                
                st.subheader(f"Tendance de {trend_param} au fil du temps")
                st.line_chart(trend_chart_data)
                
                # Ajouter des statistiques de tendance
                st.subheader("Analyse statistique de la tendance")
                
                # Calculer moyenne mobile pour montrer la tendance
                window_size = st.slider("Taille de la fen√™tre pour la moyenne mobile", 
                                      5, 100, 20, key="window_size")
                
                if len(time_trend) >= window_size:
                    time_trend['Rolling Mean'] = time_trend[trend_param].rolling(window=window_size).mean()
                    
                    # Pr√©parer les donn√©es pour le graphique
                    rolling_chart_data = pd.DataFrame({
                        trend_param: time_trend[trend_param].values,
                        f"Moyenne mobile ({window_size} points)": time_trend['Rolling Mean'].values
                    }, index=time_trend['Time'])
                    
                    st.line_chart(rolling_chart_data)
            
            # D√©tection simple d'anomalies
            st.subheader("D√©tection d'Anomalies")
            
            # M√©thode simple: Identifier les valeurs au-del√† d'un certain nombre d'√©carts-types
            std_threshold = st.slider(
                "Nombre d'√©carts-types pour d√©tecter les anomalies", 
                1.0, 5.0, 3.0, 0.1,
                key="std_threshold"
            )
            
            # Calculer la moyenne et l'√©cart-type
            if trend_step == "Toutes les √©tapes":
                # Calculer par √©tape
                step_stats = trend_data.groupby('Step')[trend_param].agg(['mean', 'std']).reset_index()
                
                # Cr√©er un conteneur pour chaque √©tape
                for step in trend_data['Step'].unique():
                    step_data = trend_data[trend_data['Step'] == step]
                    stats = step_stats[step_stats['Step'] == step].iloc[0]
                    
                    mean_val = stats['mean']
                    std_val = stats['std']
                    
                    upper_limit = mean_val + std_threshold * std_val
                    lower_limit = mean_val - std_threshold * std_val
                    
                    # Identifier les anomalies
                    anomalies = step_data[
                        (step_data[trend_param] > upper_limit) | 
                        (step_data[trend_param] < lower_limit)
                    ]
                    
                    # Afficher les r√©sultats pour cette √©tape
                    st.write(f"### √âtape: {step}")
                    st.write(f"- Moyenne: {mean_val:.2f}")
                    st.write(f"- √âcart-type: {std_val:.2f}")
                    st.write(f"- Limite sup√©rieure (+{std_threshold} œÉ): {upper_limit:.2f}")
                    st.write(f"- Limite inf√©rieure (-{std_threshold} œÉ): {lower_limit:.2f}")
                    
                    if not anomalies.empty:
                        st.warning(f"{len(anomalies)} anomalies d√©tect√©es sur {len(step_data)} points ({len(anomalies)/len(step_data)*100:.1f}%).")
                        
                        # Afficher les anomalies
                        st.dataframe(anomalies[['Batch name', 'Step', trend_param, 'Time']])
                    else:
                        st.success(f"Aucune anomalie d√©tect√©e pour l'√©tape {step}.")
                    
                    st.markdown("---")
            else:
                # Analyser une seule √©tape
                mean_val = trend_data[trend_param].mean()
                std_val = trend_data[trend_param].std()
                
                upper_limit = mean_val + std_threshold * std_val
                lower_limit = mean_val - std_threshold * std_val
                
                # Identifier les anomalies
                anomalies = trend_data[
                    (trend_data[trend_param] > upper_limit) | 
                    (trend_data[trend_param] < lower_limit)
                ]
                
                # Afficher les statistiques
                st.write(f"### Statistiques pour {trend_step}")
                st.write(f"- Moyenne: {mean_val:.2f}")
                st.write(f"- √âcart-type: {std_val:.2f}")
                st.write(f"- Limite sup√©rieure (+{std_threshold} œÉ): {upper_limit:.2f}")
                st.write(f"- Limite inf√©rieure (-{std_threshold} œÉ): {lower_limit:.2f}")
                
                # Afficher les r√©sultats
                if not anomalies.empty:
                    st.warning(f"{len(anomalies)} anomalies d√©tect√©es sur {len(trend_data)} points ({len(anomalies)/len(trend_data)*100:.1f}%).")
                    
                    # Afficher les anomalies
                    st.dataframe(anomalies[['Batch name', trend_param, 'Time']])
                    
                    # Option pour t√©l√©charger
                    csv = anomalies.to_csv(index=False)
                    st.download_button(
                        label="T√©l√©charger la liste des anomalies",
                        data=csv,
                        file_name=f"anomalies_{trend_param}_{trend_step}.csv",
                        mime='text/csv',
                    )
                else:
                    st.success(f"Aucune anomalie d√©tect√©e pour {trend_param} - {trend_step} avec un seuil de {std_threshold} √©carts-types.")
        else:
            st.warning("Veuillez s√©lectionner un param√®tre pour l'analyse.")
    
    # Nouvelle section pour la pr√©diction des comportements
    st.header("Pr√©diction des Comportements")
    st.markdown("""
    Cette section utilise diff√©rents mod√®les de r√©gression pour pr√©dire le comportement 
    futur des param√®tres √† partir des donn√©es historiques.
    """)
    
    # S√©lection du param√®tre √† pr√©dire
    pred_param = st.selectbox(
        "Param√®tre √† pr√©dire",
        options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
        index=0,
        key="pred_param"
    )
    
    # S√©lection du lot et de l'√©tape pour la pr√©diction
    col1, col2 = st.columns(2)
    with col1:
        pred_batch = st.selectbox(
            "Lot √† analyser",
            options=sorted(data['Batch name'].unique()),
            key="pred_batch"
        )
    with col2:
        pred_step = st.selectbox(
            "√âtape √† analyser",
            options=sorted(data['Step'].unique()),
            key="pred_step"
        )
    
    # Filtrer les donn√©es pour le lot et l'√©tape s√©lectionn√©s
    pred_data = data[(data['Batch name'] == pred_batch) & (data['Step'] == pred_step)]
    
    if not pred_data.empty:
        # Cr√©ation des onglets pour diff√©rents types de pr√©diction
        pred_tabs = st.tabs(["Pr√©diction Temporelle", "Pr√©diction Bas√©e sur les Corr√©lations"])
        with pred_tabs[0]:
            st.subheader("Pr√©diction Temporelle")
            st.markdown("""
            Ce mod√®le utilise la progression temporelle ou l'index pour pr√©dire l'√©volution future du param√®tre s√©lectionn√©.
            """)
            
            # R√©initialiser l'index pour la progression lin√©aire
            pred_data_reset = pred_data.reset_index(drop=True)
            x_values = np.array(range(len(pred_data_reset)))
            y_values = pred_data_reset[pred_param].values
            
            # S√©lection du mod√®le et de ses param√®tres
            model_type = st.selectbox(
                "Type de mod√®le de pr√©diction",
                options=["R√©gression Lin√©aire", "R√©gression Polynomiale", "XGBoost"],
                key="pred_model_type"
            )
            
            if model_type == "R√©gression Polynomiale":
                degree = st.slider(
                    "Degr√© du polyn√¥me",
                    min_value=1,
                    max_value=5,
                    value=2,
                    key="pred_poly_degree"
                )
            else:
                degree = 1
                
            # Param√®tres sp√©cifiques √† XGBoost
            if model_type == "XGBoost":
                col1, col2 = st.columns(2)
                with col1:
                    n_estimators = st.slider("Nombre d'arbres", 10, 200, 100, 10, key="xgb_n_estimators")
                with col2:
                    max_depth = st.slider("Profondeur maximale des arbres", 1, 10, 3, 1, key="xgb_max_depth")
                learning_rate = st.slider("Taux d'apprentissage", 0.01, 0.3, 0.1, 0.01, key="xgb_learning_rate")
            
            # Pourcentage de donn√©es √† utiliser pour l'entra√Ænement
            train_pct = st.slider(
                "Pourcentage de donn√©es pour l'entra√Ænement",
                min_value=50,
                max_value=90,
                value=70,
                step=5,
                key="train_pct"
            )
            
            # Calculer le point de s√©paration
            split_idx = int(len(x_values) * train_pct / 100)
            x_train, x_test = x_values[:split_idx], x_values[split_idx:]
            y_train, y_test = y_values[:split_idx], y_values[split_idx:]
            
            # Ajuster le mod√®le selon le type s√©lectionn√©
            if model_type == "XGBoost":
                # Reformater les donn√©es pour XGBoost
                X_train_reshaped = x_train.reshape(-1, 1)
                X_test_reshaped = x_test.reshape(-1, 1)
                
                # Cr√©er et entra√Æner le mod√®le XGBoost
                model_xgb = xgb.XGBRegressor(
                    n_estimators=n_estimators,
                    max_depth=max_depth,
                    learning_rate=learning_rate,
                    objective='reg:squarederror',
                    random_state=42
                )
                
                model_xgb.fit(X_train_reshaped, y_train)
                
                # Pr√©dictions
                y_pred_train = model_xgb.predict(X_train_reshaped)
                y_pred_test = model_xgb.predict(X_test_reshaped)
                
                # D√©finir une fonction pour les pr√©dictions futures
                def predict_future(x_future):
                    return model_xgb.predict(x_future.reshape(-1, 1))
                
                # √âquation du mod√®le (simplifi√©e pour XGBoost)
                equation = f"XGBoost (n_estimators={n_estimators}, max_depth={max_depth}, learning_rate={learning_rate})"
            else:
                # Mod√®le polynomial ou lin√©aire (code existant)
                model_coeffs = np.polyfit(x_train, y_train, degree)
                model = np.poly1d(model_coeffs)
                
                # Pr√©dictions
                y_pred_train = model(x_train)
                y_pred_test = model(x_test)
                
                # D√©finir une fonction pour les pr√©dictions futures
                def predict_future(x_future):
                    return model(x_future)
                
                # √âquation du mod√®le
                if degree == 1:
                    equation = f"y = {model_coeffs[0]:.4f}x + {model_coeffs[1]:.4f}"
                else:
                    equation = f"Polyn√¥me de degr√© {degree}: y = "
                    for i, coef in enumerate(model_coeffs):
                        power = degree - i
                        if power == 0:
                            equation += f"{coef:.4f}"
                        elif power == 1:
                            equation += f"{coef:.4f}x + "
                        else:
                            equation += f"{coef:.4f}x^{power} + "
            
            # Calculer l'erreur (RMSE et R¬≤)
            if len(y_test) > 0:
                rmse = np.sqrt(np.mean((y_test - y_pred_test) ** 2))
                
                # Calculer R¬≤ manuellement
                y_test_mean = np.mean(y_test)
                ss_total = np.sum((y_test - y_test_mean) ** 2)
                ss_residual = np.sum((y_test - y_pred_test) ** 2)
                r2 = 1 - (ss_residual / ss_total) if ss_total != 0 else 0
                
                # Afficher les m√©triques
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Erreur quadratique moyenne (RMSE)", f"{rmse:.4f}")
                with col2:
                    st.metric("Coefficient de d√©termination (R¬≤)", f"{r2:.4f}")
            
            # Extrapolation pour pr√©dire l'avenir
            future_points = st.slider(
                "Nombre de points √† pr√©dire dans le futur",
                min_value=0,
                max_value=int(len(x_values) * 0.5),
                value=int(len(x_values) * 0.2),
                key="future_points"
            )
            
            if future_points > 0:
                # G√©n√©rer les points futurs
                x_future = np.array(range(len(x_values), len(x_values) + future_points))
                y_future = predict_future(x_future)
                
                # Pr√©parer les donn√©es pour la visualisation
                train_df = pd.DataFrame({
                    'Index': x_train,
                    'Valeur r√©elle': y_train,
                    'Pr√©diction': y_pred_train
                })
                
                test_df = pd.DataFrame({
                    'Index': x_test,
                    'Valeur r√©elle': y_test,
                    'Pr√©diction': y_pred_test
                })
                
                future_df = pd.DataFrame({
                    'Index': x_future,
                    'Pr√©diction': y_future
                })
                
                # Afficher les r√©sultats sous forme de tableau
                st.subheader("Donn√©es d'entra√Ænement et pr√©dictions")
                
                # Afficher l'√©quation du mod√®le
                st.write(f"**√âquation du mod√®le:** {equation}")
                
                # Combiner les donn√©es pour le graphique
                viz_data = pd.DataFrame()
                viz_data['Index'] = list(x_train) + list(x_test) + list(x_future)
                
                # Ajouter les valeurs r√©elles (avec NaN pour les points futurs)
                real_values = list(y_train) + list(y_test) + [np.nan] * len(y_future)
                viz_data['Valeur r√©elle'] = real_values
                
                # Ajouter les valeurs pr√©dites
                pred_values = list(y_pred_train) + list(y_pred_test) + list(y_future)
                viz_data['Pr√©diction'] = pred_values
                
                # Afficher le graphique des pr√©dictions vs r√©alit√©
                st.line_chart(viz_data.set_index('Index'))
                
                # Afficher les pr√©dictions futures
                st.subheader("Valeurs Pr√©dites pour le Futur")
                st.dataframe(future_df)
                
                # Option de t√©l√©chargement
                csv = future_df.to_csv(index=False)
                st.download_button(
                    label="T√©l√©charger les pr√©dictions",
                    data=csv,
                    file_name=f"predictions_{pred_param}_{pred_batch}_{pred_step}.csv",
                    mime='text/csv',
                )
        
        with pred_tabs[1]:
            st.subheader("Pr√©diction Bas√©e sur les Corr√©lations")
            st.markdown("""
            Ce mod√®le utilise les corr√©lations entre diff√©rents param√®tres pour pr√©dire le param√®tre cible.
            """)
            
            # S√©lection des variables explicatives
            feature_vars = st.multiselect(
                "Variables explicatives",
                options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time', pred_param]],
                default=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time', pred_param]][:2],
                key="corr_feature_vars"
            )
            
            if feature_vars and len(feature_vars) > 0:
                # Pr√©paration des donn√©es
                X = pred_data_reset[feature_vars].values
                y = pred_data_reset[pred_param].values
                
                # V√©rifier s'il y a des valeurs manquantes
                valid_mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
                X_valid = X[valid_mask]
                y_valid = y[valid_mask]
                
                if len(X_valid) > 0:
                    # Calculer le point de s√©paration
                    split_idx = int(len(X_valid) * train_pct / 100)
                    X_train, X_test = X_valid[:split_idx], X_valid[split_idx:]
                    y_train, y_test = y_valid[:split_idx], y_valid[split_idx:]
                    
                    # V√©rification pour √©viter les erreurs
                    if X_train.shape[0] > 0 and X_test.shape[0] > 0:
                        # Ajuster un mod√®le de r√©gression lin√©aire multiple
                        # Ajouter une constante (terme d'interception)
                        X_train_with_const = np.column_stack((np.ones(X_train.shape[0]), X_train))
                        
                        # R√©soudre l'√©quation lin√©aire
                        try:
                            # Utiliser np.linalg.lstsq qui est plus stable que np.linalg.solve
                            coeffs, residuals, rank, s = np.linalg.lstsq(X_train_with_const, y_train, rcond=None)
                            
                            # Faire des pr√©dictions
                            X_test_with_const = np.column_stack((np.ones(X_test.shape[0]), X_test))
                            y_pred = X_test_with_const @ coeffs
                            
                            # Calculer l'erreur (RMSE et R¬≤)
                            rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
                            
                            # Calculer R¬≤ manuellement
                            y_test_mean = np.mean(y_test)
                            ss_total = np.sum((y_test - y_test_mean) ** 2)
                            ss_residual = np.sum((y_test - y_pred) ** 2)
                            r2 = 1 - (ss_residual / ss_total) if ss_total != 0 else 0
                            
                            # Afficher les m√©triques
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("Erreur quadratique moyenne (RMSE)", f"{rmse:.4f}")
                            with col2:
                                st.metric("Coefficient de d√©termination (R¬≤)", f"{r2:.4f}")
                            
                            # Afficher l'√©quation du mod√®le
                            equation = f"{pred_param} = {coeffs[0]:.4f}"
                            for i, feature in enumerate(feature_vars):
                                equation += f" + {coeffs[i+1]:.4f} √ó {feature}"
                            
                            st.markdown(f"**√âquation du mod√®le:**")
                            st.markdown(f"`{equation}`")
                            
                            # Cr√©er un tableau pour comparer les pr√©dictions et les valeurs r√©elles
                            prediction_df = pd.DataFrame({
                                'Valeur r√©elle': y_test,
                                'Pr√©diction': y_pred,
                                'Diff√©rence': y_test - y_pred
                            })
                            
                            st.subheader("Comparaison des pr√©dictions et valeurs r√©elles")
                            st.dataframe(prediction_df)
                            
                            # Afficher l'importance des variables
                            importance = np.abs(coeffs[1:])
                            importance_norm = importance / np.sum(importance)
                            importance_df = pd.DataFrame({
                                'Variable': feature_vars,
                                'Coefficient': coeffs[1:],
                                'Importance': importance_norm
                            }).sort_values(by='Importance', ascending=False)
                            
                            st.subheader("Importance des Variables")
                            st.dataframe(importance_df)
                            
                            # Version graphique simple de l'importance des variables
                            st.bar_chart(importance_df.set_index('Variable')['Importance'])
                            
                            # Permettre √† l'utilisateur de faire des pr√©dictions pour de nouvelles valeurs
                            st.subheader("Faire une pr√©diction avec de nouvelles valeurs")
                            
                            # Cr√©er des sliders pour chaque variable
                            new_values = {}
                            for feature in feature_vars:
                                min_val = float(pred_data_reset[feature].min())
                                max_val = float(pred_data_reset[feature].max())
                                default_val = float(pred_data_reset[feature].mean())
                                
                                new_values[feature] = st.slider(
                                    f"Valeur pour {feature}",
                                    min_value=min_val,
                                    max_value=max_val,
                                    value=default_val,
                                    key=f"slider_{feature}"
                                )
                            
                            # Calculer la pr√©diction pour les nouvelles valeurs
                            new_X = np.array([new_values[feature] for feature in feature_vars])
                            new_X_with_const = np.append(1, new_X)
                            new_prediction = new_X_with_const @ coeffs
                            
                            st.success(f"**Pr√©diction pour {pred_param}:** {new_prediction:.4f}")
                            
                        except Exception as e:
                            st.error(f"Erreur lors de l'ajustement du mod√®le : {e}")
                    else:
                        st.warning("Pas assez de donn√©es pour diviser en ensembles d'entra√Ænement et de test.")
                else:
                    st.warning("Les donn√©es contiennent trop de valeurs manquantes pour ajuster un mod√®le.")
            else:
                st.warning("Veuillez s√©lectionner au moins une variable explicative.")
    else:
        st.warning("Pas de donn√©es disponibles pour le lot et l'√©tape s√©lectionn√©s.")
    
    # Section d'aide √† la d√©cision
    st.header("Aide √† la D√©cision")
    
    # Recommandations bas√©es sur l'analyse
    st.subheader("Recommandations pour l'Am√©lioration des Proc√©d√©s")
    st.markdown("""
    Sur la base de l'analyse des donn√©es, voici quelques recommandations pour am√©liorer 
    les proc√©d√©s de production et r√©duire les d√©viations.
    """)
    
    # G√©n√©rer des recommandations bas√©es sur les donn√©es
    recommendations = [
        "**Surveillez √©troitement les temp√©ratures** pendant les phases critiques du processus, en particulier pendant la phase de r√©action.",
        "**Standardisez les proc√©dures de contr√¥le** pour maintenir des conditions constantes entre les lots.",
        "**√âtablissez des limites d'alerte** bas√©es sur les d√©viations statistiques observ√©es dans les lots historiques.",
        "**Formez les op√©rateurs** √† reconna√Ætre rapidement les signes de d√©viation et √† prendre des mesures correctives.",
        "**Documentez syst√©matiquement** toutes les interventions manuelles pendant le processus de production."
    ]
    
    for i, rec in enumerate(recommendations):
        st.markdown(f"{i+1}. {rec}")
    
    # Ajouter une section pour les notes personnalis√©es
    st.subheader("Notes et Observations")
    user_notes = st.text_area(
        "Ajoutez vos propres observations et recommandations",
        height=150
    )
    
    if st.button("Sauvegarder les notes"):
        st.success("Notes sauvegard√©es avec succ√®s!")
        
        # Cr√©ation d'un rapport combinant l'analyse et les notes
        if user_notes:
            report = f"""
            # Rapport d'Analyse des Proc√©d√©s - {datetime.now().strftime("%Y-%m-%d")}
            
            ## Recommandations Syst√®me
            
            {chr(10).join([f"- {rec}" for rec in recommendations])}
            
            ## Notes et Observations
            
            {user_notes}
            """
            
            # Option pour t√©l√©charger le rapport
            st.download_button(
                label="T√©l√©charger le rapport",
                data=report,
                file_name=f"rapport_analyse_{datetime.now().strftime('%Y%m%d')}.md",
                mime='text/markdown',
            )

# Pied de page
st.markdown("---")
st.markdown("""
**Application d√©velopp√©e pour Sanofi** | Version 1.0  
Cette application permet d'identifier les d√©viations de proc√©d√©s en √©tudiant l'√©volution des param√®tres de production.
""")