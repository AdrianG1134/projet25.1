import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import xgboost as xgb 
import plotly.express as px
import plotly.graph_objects as go

# Configuration de la page
st.set_page_config(
    page_title="Analyse des proc√©d√©s Sanofi",
    page_icon="üíä",
    layout="wide"
)

# Titre et description
st.markdown("<h1 style='text-align: center;'><em>Datizzüíä</em></h1>", unsafe_allow_html=True)
st.markdown("""
*Datizz* est une application permettant de visualiser et analyser les donn√©es de capteurs pour les lots de production pharmaceutique.
""")

# Fonction pour charger les donn√©es
@st.cache_data
def load_data(file_path):
    try:
        data = pd.read_csv(file_path,sep=',')
        if 'Time' in data.columns:
            data['Time'] = pd.to_datetime(data['Time'])
        return data
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es: {e}")
        return None

# Initialisation des √©tats de session
def initialize_session_states():
    if 'current_lot_index' not in st.session_state:
        st.session_state.current_lot_index = 0
    if 'selected_data' not in st.session_state:
        st.session_state.selected_data = pd.DataFrame()
    if 'selections' not in st.session_state:
        st.session_state.selections = {}
    if 'mouse_selections' not in st.session_state:
        st.session_state.mouse_selections = {}
initialize_session_states()

# Fonction pour ajouter une s√©lection de segment
def add_selected_range(lot_data, start_index, end_index, lot):
    if 'selected_data' not in st.session_state:
        st.session_state.selected_data = pd.DataFrame()
    if start_index <= end_index:
        selected_range = lot_data.iloc[start_index:end_index + 1].copy()
        selected_range[st.session_state.lot_column] = lot  
        st.session_state.selection_counter = st.session_state.get("selection_counter", 0) + 1
        sel_id = st.session_state.selection_counter
        selected_range["Selection ID"] = sel_id

        if st.session_state.selected_data.empty:
            st.session_state.selected_data = selected_range
        else:
            st.session_state.selected_data = pd.concat([st.session_state.selected_data, selected_range], ignore_index=True)

        new_summary = {
            "Selection ID": sel_id,
            "Lot": lot,
            "Start Index": start_index,
            "End Index": end_index
        }
        if "selection_summary" not in st.session_state:
            st.session_state.selection_summary = [new_summary]
        else:
            st.session_state.selection_summary.append(new_summary)
        
        return f"Plage s√©lectionn√©e pour le lot {lot} (ID {sel_id}) ajout√©e."
    else:
        return "L'indice de d√©but doit √™tre inf√©rieur √† l'indice de fin."

# Sidebar pour le chargement des donn√©es
with st.sidebar:
    st.header("Configuration")
    uploaded_file = st.file_uploader("Charger le fichier CSV des donn√©es", type=['csv'])
    if uploaded_file is not None:
        data = load_data(uploaded_file)
    else:
        st.info("Veuillez charger votre fichier CSV pour commencer l'analyse.")

if 'data' in locals() and data is not None:
    # V√©rification des valeurs manquantes
    missing_values = data.isnull().sum().sum()
    if missing_values > 0:
        with st.expander(f"‚ö†Ô∏è {missing_values} valeur(s) manquante(s) d√©tect√©e(s)"):
            st.write("Voici les lignes avec des valeurs manquantes :")
            st.dataframe(data[data.isnull().any(axis=1)])
    else:
        st.success("Aucune valeur manquante d√©tect√©e.")

    # Cr√©ation des 3 onglets principaux
    main_tabs = st.tabs(["Visualisation", "Analyse Statistique", "Pr√©diction"])

    # -----------------------------------
    # Onglet 1 : Visualisation
    # -----------------------------------
    with main_tabs[0]:
        st.header("Visualisation des Lots")
        
        # Exploration des donn√©es
        st.subheader("Exploration des donn√©es")
        st.write(f"Nombre total d'observations: {data.shape[0]}")
        st.write(f"Nombre de lots: {data['Batch name'].nunique()}")
        st.write(f"√âtapes du proc√©d√©: {', '.join(data['Step'].unique())}")
        if st.checkbox("Afficher un aper√ßu des donn√©es"):
            st.dataframe(data.head(10))
        
        # Cr√©ation des sous-onglets dans Visualisation
        vis_tabs = st.tabs(["Visualisation Individuelle", "D√©coupage & Superposition", "Analyse Comparative", "Comparaison Courbe"])
        
        # --- Sous-onglet 1 : Visualisation Individuelle ---
        with vis_tabs[0]:
            st.subheader("Visualisation Individuel")
            
            # S√©lection du lot et de l'√©tape
            col1, col2 = st.columns(2)
            with col1:
                selected_batch = st.selectbox("S√©lectionner un lot", options=sorted(data['Batch name'].unique()), key="vis_batch")
            with col2:
                selected_step = st.selectbox("S√©lectionner une √©tape", 
                                           options=["Toutes les √©tapes"] + sorted(data['Step'].unique()), key="vis_step")
            
            # Filtrage des donn√©es
            if selected_step == "Toutes les √©tapes":
                filtered_data = data[data['Batch name'] == selected_batch]
            else:
                filtered_data = data[(data['Batch name'] == selected_batch) & (data['Step'] == selected_step)]
                
            if not filtered_data.empty:
                # S√©lection des param√®tres √† visualiser
                params = st.multiselect(
                    "S√©lectionner les param√®tres √† visualiser",
                    options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
                    default=['Temp√©rature fond de cuve', 'Temp√©rature haut de colonne', 'Temp√©rature r√©acteur'],
                    key="vis_params"
                )
                
                if params:
                    # Cr√©ation du graphique avec Streamlit native
                    if 'Time' in filtered_data.columns:
                        chart_data = filtered_data.set_index('Time')[params]
                    else:
                        chart_data = filtered_data[params]
                    
                    st.line_chart(chart_data)
                    
                    # Option pour t√©l√©charger les donn√©es filtr√©es
                    csv = filtered_data.to_csv(index=False)
                    st.download_button(
                        label="T√©l√©charger les donn√©es filtr√©es",
                        data=csv,
                        file_name=f"{selected_batch}_{selected_step.replace(' ', '_')}.csv",
                        mime='text/csv',
                    )
                else:
                    st.warning("Veuillez s√©lectionner au moins un param√®tre √† visualiser.")
            else:
                st.warning("Aucune donn√©e disponible pour ce lot et cette √©tape.")




        # --- Sous-onglet 3 : Analyse Comparative ---
        with vis_tabs[2]:
            st.write("Test Analyse Comparative")
            st.subheader("Analyse Comparative")
            
            # Fonction pour obtenir les statistiques
            def get_stats(df, batch_name):
                if option == "Clustering sur la distribution des temp√©ratures":
                    df = df.drop(columns=['Time', "IMPURETE_A", "IMPURETE_B", "IMPURETE_C","IMPURITY_BATCH", "Niveau de la cuve", "Vitesse d'agitation", "Step"], errors='ignore')  # Exclure la colonne 'Time' si elle existe
                else:
                    df = df.drop(columns=['Time', "Temp√©rature fond de cuve", "Temp√©rature haut de colonne", "Temp√©rature r√©acteur","IMPURITY_BATCH", "Niveau de la cuve", "Vitesse d'agitation", "Step"], errors='ignore')
                stats = df.describe().T
                stats["Batch"] = batch_name
                return stats.reset_index()  # R√©initialisation de l'index pour un affichage propre
                
            st.write("Test Analyse Comparative")
            st.markdown("### Clustering des Batchs")

            # Menu d√©pliant pour choisir l'option de clustering
            with st.expander("Choisissez votre option de clustering"):
                option = st.radio(
                    "Sur quelles variables souhaitez-vous effectuer le clustering ?",
                    options=["Clustering sur la distribution des temp√©ratures", "Clustering sur les taux d'impuret√©"]
                )
                
            batch_names = data['Batch name'].dropna().unique()
            all_stats = [get_stats(data[data['Batch name'] == batch], batch) for batch in batch_names]
            stats_all_batches = pd.concat(all_stats, ignore_index=True)
            
            stats_pivoted = stats_all_batches.pivot(index='Batch', columns='index', values=['mean', 'std', 'min', '25%', '50%', '75%', 'max'])
            stats_pivoted.columns = ['_'.join(col) for col in stats_pivoted.columns]
            stats_pivoted = stats_pivoted.reset_index()
            X_data = stats_pivoted.drop(columns=["Batch"]).apply(pd.to_numeric, errors='coerce').to_numpy()
            
            # Impl√©mentation simple de K-Means
            def simple_kmeans(X, k=3, max_iters=100):
                np.random.seed(42)
                centroids = X[np.random.choice(len(X), k, replace=False)]
                for _ in range(max_iters):
                    distances = np.linalg.norm(X[:, None] - centroids[None, :], axis=2)
                    labels = np.argmin(distances, axis=1)
                    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])
                    if np.all(centroids == new_centroids):
                        break
                    centroids = new_centroids
                return labels
            
            k = st.slider("Nombre de clusters (K)", min_value=2, max_value=10, value=3)
            labels = simple_kmeans(X_data, k)
            stats_pivoted["Cluster"] = labels
            
            # üîπ R√©duction de dimension avec PCA (manuelle, sans scipy)
            mean = X_data.mean(axis=0)
            std = X_data.std(axis=0)
            std[std == 0] = 1  # √âviter la division par z√©ro pour les colonnes constantes
            X_scaled = (X_data - mean) / std
            
            if np.isnan(X_scaled).sum() > 0 or np.isinf(X_scaled).sum() > 0:
                X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1e10, neginf=-1e10)
            try:
                U, S, Vt = np.linalg.svd(X_scaled, full_matrices=False)
                X_pca = U[:, :2] * S[:2]
            except np.linalg.LinAlgError:
                st.error("La d√©composition SVD a √©chou√©.")
                
            explained_variance_ratio = (S**2) / np.sum(S**2)
            variance_expliquee = np.sum(explained_variance_ratio[:2]) * 100
            st.markdown(f"Les deux premi√®res composantes expliquent {variance_expliquee:.2f}% de la variance totale.")
            
            df_clusters = pd.DataFrame({
                "Batch": stats_pivoted["Batch"],
                "PC1": X_pca[:, 0],
                "PC2": X_pca[:, 1],
                "Cluster": labels
            })
            
            fig = go.Figure()
            
            # Ajouter une trace pour chaque cluster
            for cluster_num in df_clusters['Cluster'].unique():
                cluster_data = df_clusters[df_clusters['Cluster'] == cluster_num]
                fig.add_trace(go.Scatter(
                    x=cluster_data['PC1'], 
                    y=cluster_data['PC2'], 
                    mode='markers', 
                    marker=dict(size=8),
                    name=f"Cluster {cluster_num}",  # Nom de la l√©gende pour ce cluster
                    text=cluster_data['Batch'],  # Texte affich√© au survol
                    hoverinfo='text',  # Affiche uniquement le texte au survol
                    hovertemplate="Batch: %{text}<br>PC1: %{x}<br>PC2: %{y}<extra></extra>"
                ))
            
            fig.update_layout(
                title="Clustering des Batchs",
                xaxis_title="Composante principale 1",
                yaxis_title="Composante principale 2",
                legend_title="Clusters", 
                dragmode='select',  # Permet de s√©lectionner une zone
                selectdirection='any',  # Permet la s√©lection dans n'importe quelle direction
            )
            event = st.plotly_chart(fig, key="scatter_plot", on_select="rerun", selection_mode=("points", "box", "lasso"), use_container_width=True)
            
            if event and "selection" in event:
                selection_obj = event["selection"]
                selected_points = selection_obj.get("points", [])
                if selected_points:
                    selected_batches = [pt["text"] for pt in selected_points]
                    st.success(f"Batchs s√©lectionn√©s : {', '.join(selected_batches)}")
                    filtered_data = data[data['Batch name'].isin(selected_batches)]
                    st.write("Donn√©es des batchs s√©lectionn√©s :")
                    st.dataframe(filtered_data)
                    overlay_param = st.selectbox("Param√®tre √† comparer", options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']], key="overlay_param_clustering")
                    overlay_data = pd.DataFrame()
                    for batch in selected_batches:
                        batch_data = data[data['Batch name'] == batch]
                        batch_series = pd.Series(batch_data[overlay_param].values)
                        if overlay_data.empty:
                            overlay_data = pd.DataFrame({batch: batch_series})
                        else:
                            max_len = max(len(overlay_data), len(batch_series))
                            overlay_data = overlay_data.reindex(range(max_len), fill_value=np.nan)
                            batch_series = batch_series.reindex(range(max_len), fill_value=np.nan)
                            overlay_data[batch] = batch_series
                    if not overlay_data.empty:
                        st.line_chart(overlay_data)
                else:
                    st.info("S√©lectionne des points sur le graphique.")
            with st.expander("Afficher les donn√©es d'un cluster"):
                cluster_selection = st.selectbox("S√©lectionnez un cluster", sorted(df_clusters["Cluster"].unique()))
                selected_batches = df_clusters[df_clusters["Cluster"] == cluster_selection]["Batch"]
                filtered_data = data[data["Batch name"].isin(selected_batches)]
                st.subheader(f"Batchs du Cluster {cluster_selection}")
                st.dataframe(filtered_data)
                
            st.subheader("Analyse Comparative et D√©tection des D√©viations")
            col1, col2 = st.columns(2)
            with col1:
                ideal_batch = st.selectbox("Lot de r√©f√©rence (id√©al)", options=sorted(data['Batch name'].unique()), key="ideal_batch")
            with col2:
                compare_batch = st.selectbox("Lot √† comparer", options=[b for b in sorted(data['Batch name'].unique()) if b != ideal_batch], key="compare_batch")
            compare_step = st.selectbox("√âtape √† comparer", options=sorted(data['Step'].unique()), key="compare_step")
            if ideal_batch and compare_batch and compare_step:
                ideal_data = data[(data['Batch name'] == ideal_batch) & (data['Step'] == compare_step)]
                compare_data = data[(data['Batch name'] == compare_batch) & (data['Step'] == compare_step)]
                if not ideal_data.empty and not compare_data.empty:
                    compare_params = st.multiselect("Param√®tres √† comparer", options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']], default=['Temp√©rature fond de cuve', 'Temp√©rature haut de colonne'], key="compare_params")
                    if compare_params:
                        for param in compare_params:
                            ideal_data_reset = ideal_data.reset_index(drop=True)
                            compare_data_reset = compare_data.reset_index(drop=True)
                            min_len = min(len(ideal_data_reset), len(compare_data_reset))
                            ideal_series = ideal_data_reset[param].iloc[:min_len]
                            compare_series = compare_data_reset[param].iloc[:min_len]
                            diff = abs(ideal_series.values - compare_series.values)
                            comparison_df = pd.DataFrame({
                                f"{ideal_batch} (R√©f√©rence)": ideal_series.values,
                                f"{compare_batch}": compare_series.values,
                                "Diff√©rence absolue": diff
                            })
                            st.subheader(f"Comparaison de {param} - {compare_step}")
                            st.line_chart(comparison_df)
                            threshold = st.slider(f"Seuil de d√©viation pour {param}", 0.0, float(max(diff)*1.5), float(max(diff)*0.2), key=f"threshold_{param}")
                            deviation_indices = np.where(diff > threshold)[0]
                            if len(deviation_indices) > 0:
                                ranges = []
                                start = deviation_indices[0]
                                for i in range(1, len(deviation_indices)):
                                    if deviation_indices[i] != deviation_indices[i-1] + 1:
                                        ranges.append((start, deviation_indices[i-1]))
                                        start = deviation_indices[i]
                                ranges.append((start, deviation_indices[-1]))
                                deviation_df = pd.DataFrame({
                                    f"{ideal_batch} (R√©f√©rence)": ideal_series.values,
                                    f"{compare_batch}": compare_series.values,
                                    "Diff√©rence absolue": diff,
                                    "Seuil": [threshold]*len(diff)
                                })
                                st.subheader(f"D√©viations pour {param}")
                                st.line_chart(deviation_df)
                                st.warning(f"D√©viations d√©tect√©es pour {param}: {len(deviation_indices)} points d√©passent le seuil.")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("D√©viation max", f"{max(diff):.2f}")
                                with col2:
                                    st.metric("D√©viation moyenne", f"{np.mean(diff):.2f}")
                                with col3:
                                    st.metric("% de points d√©v.", f"{len(deviation_indices)/min_len*100:.1f}%")
                                deviation_points = pd.DataFrame({
                                    "Index": deviation_indices,
                                    f"{ideal_batch} (R√©f√©rence)": ideal_series.iloc[deviation_indices].values,
                                    f"{compare_batch}": compare_series.iloc[deviation_indices].values,
                                    "Diff√©rence": diff[deviation_indices]
                                })
                                st.subheader("Points de d√©viation")
                                st.dataframe(deviation_points)
                            else:
                                st.success(f"Aucune d√©viation significative pour {param}.")
                    else:
                        st.warning("S√©lectionne au moins un param√®tre.")
                else:
                    st.warning("Donn√©es insuffisantes pour l'un des lots.")
            else:
                st.info("S√©lectionne un lot de r√©f√©rence, un lot √† comparer et une √©tape.")

        with vis_tabs[1]:
            # -------------------------------
            # Onglets pour D√©coupage et Superposition
            # -------------------------------
            vis_tab = st.tabs(["D√©coupage", "Superposition"])
        
            # === Onglet D√©coupage ===
            with vis_tab[0]:
                st.header("D√©coupage de courbes")
        
                # R√©cup√©ration de la liste des colonnes
                all_columns = data.columns.tolist()
                
                # D√©finition des colonnes par d√©faut
                default_lot = "Batch name" if "Batch name" in all_columns else all_columns[0]
                default_date = "Time" if "Time" in all_columns else all_columns[1]
                default_target = "Temp√©rature fond de cuve" if "Temp√©rature fond de cuve" in all_columns else all_columns[2]
                
                # S√©lecteurs pour lot/date/cible
                lot_column = st.selectbox("Colonne du lot", all_columns, index=all_columns.index(default_lot))
                date_column = st.selectbox("Colonne de date", all_columns, index=all_columns.index(default_date))
                target_column = st.selectbox("Colonne √† d√©couper (cible)", all_columns, index=all_columns.index(default_target))
                overlay_columns = st.multiselect(
                    "Colonnes suppl√©mentaires √† superposer",
                    [col for col in all_columns if col not in [lot_column, date_column, target_column]]
                )
                
                # Stocker les infos dans st.session_state pour pouvoir les r√©utiliser
                st.session_state["lot_column"] = lot_column
                st.session_state["date_column"] = date_column
                st.session_state["target_column"] = target_column
                st.session_state["overlay_columns"] = overlay_columns
        
                st.markdown("#### Visualisation interactive pour d√©coupage")
        
                # Forcer le type str pour la colonne lot (au cas o√π)
                data[lot_column] = data[lot_column].astype(str)
        
                # Lister les lots et g√©rer un index pour naviguer
                lots = sorted(data[lot_column].unique())
                if "current_lot_index" not in st.session_state:
                    st.session_state.current_lot_index = 0
        
                col_prev, col_next = st.columns(2)
                with col_prev:
                    if st.button("Lot pr√©c√©dent"):
                        st.session_state.current_lot_index = max(0, st.session_state.current_lot_index - 1)
                with col_next:
                    if st.button("Lot suivant"):
                        st.session_state.current_lot_index = min(len(lots) - 1, st.session_state.current_lot_index + 1)
        
                # S√©lecteur pour chercher directement un lot
                selected_lot = st.selectbox("Rechercher un lot", options=lots, index=st.session_state.current_lot_index)
                if lots[st.session_state.current_lot_index] != selected_lot:
                    st.session_state.current_lot_index = lots.index(selected_lot)
        
                # Filtrer les donn√©es du lot courant
                current_lot = lots[st.session_state.current_lot_index]
                lot_data = data[data[lot_column] == current_lot].copy().reset_index(drop=True)
        
                # Convertir la colonne de date en datetime et trier
                lot_data[date_column] = pd.to_datetime(lot_data[date_column], errors='coerce')
                lot_data = lot_data.sort_values(by=date_column).reset_index(drop=True)
        
                # (Optionnel) S√©lecteur d'impuret√©, si tu en as
                impurity_columns = [col for col in lot_data.columns if "impurete" in col.lower()]
                if impurity_columns:
                    selected_impurity = st.selectbox(
                        "Choisir l'impuret√© pour afficher la valeur (d√©coupage)",
                        options=impurity_columns,
                        index=0
                    )
                    if selected_impurity:
                        # Exemple : on affiche juste la valeur moyenne ou la 1√®re valeur
                        avg_impurity = lot_data[selected_impurity].mean()
                        st.write(f"Impuret√© moyenne pour le lot {current_lot} : {avg_impurity:.2f}")
                else:
                    st.info("Aucune colonne d'impuret√© trouv√©e dans les donn√©es.")
        
                # Cr√©ation du graphique interactif
                fig = go.Figure(data=go.Scatter(
                    x=lot_data[date_column],
                    y=lot_data[target_column],
                    mode='lines+markers',
                    marker=dict(size=4),
                    name=f"{target_column} - Lot {current_lot}",
                    customdata=lot_data.index,
                    hovertemplate=(
                        "Index: %{customdata}<br>"
                        f"{date_column}: %{{x}}<br>"
                        f"{target_column}: %{{y}}<extra></extra>"
                    ),
                    line=dict(color='blue')  # Ex: bleu ciel
                ))
                fig.update_layout(
                    title=f"Graphique interactif pour le lot {current_lot}",
                    xaxis_title=date_column,
                    yaxis_title=target_column,
                    height=600
                )
        
                # Affichage du graphique avec s√©lection interactive
                event = st.plotly_chart(
                    fig,
                    key="decoupage_chart",
                    on_select="rerun",
                    selection_mode=("points", "box", "lasso"),
                    use_container_width=True
                )
        
                # R√©cup√©rer l'√©tat de s√©lection et extraire la plage
                if event and "selection" in event:
                    selection_obj = event["selection"]
                    selected_points = selection_obj.get("points", [])
                    if selected_points:
                        indices = [pt["point_index"] for pt in selected_points]
                        start_idx = min(indices)
                        end_idx = max(indices)
                        st.success(f"Plage s√©lectionn√©e : de l'index {start_idx} √† {end_idx}")
                        if st.button("Valider la s√©lection interactive"):
                            message = add_selected_range(lot_data, int(start_idx), int(end_idx), current_lot)
                            st.success(message)
                    else:
                        st.info("Aucune s√©lection d√©tect√©e. S√©lectionnez des points (Box ou Lasso) sur le graphique.")
                else:
                    st.info("Effectuez une s√©lection sur le graphique pour d√©couper.")
        
                st.markdown("### Sections s√©lectionn√©es")
                # Affichage du r√©sum√© des s√©lections
                if "selection_summary" in st.session_state and st.session_state.selection_summary:
                    summary_df = pd.DataFrame(st.session_state.selection_summary)
                    st.dataframe(summary_df)
        
                    # Suppression de s√©lections
                    selections_to_delete = st.multiselect(
                        "S√©lectionnez les s√©lections √† supprimer",
                        options=summary_df["Selection ID"].tolist(),
                        format_func=lambda x: (
                            f"ID {x} - Lot {summary_df.loc[summary_df['Selection ID'] == x, 'Lot'].values[0]} "
                            f"(de {summary_df.loc[summary_df['Selection ID'] == x, 'Start Index'].values[0]} "
                            f"√† {summary_df.loc[summary_df['Selection ID'] == x, 'End Index'].values[0]})"
                        )
                    )
                    if selections_to_delete and st.button("Supprimer les s√©lections"):
                        st.session_state.selection_summary = [
                            s for s in st.session_state.selection_summary if s["Selection ID"] not in selections_to_delete
                        ]
                        if not st.session_state.selected_data.empty:
                            st.session_state.selected_data = st.session_state.selected_data[
                                ~st.session_state.selected_data["Selection ID"].isin(selections_to_delete)
                            ]
                        st.success("S√©lections supprim√©es.")
                        #st.experimental_rerun()
        
                    # Export des s√©lections valid√©es
                    export_filename = st.text_input("Nom du fichier CSV √† exporter :", value="extractions.csv")
                    if st.button("Exporter toutes les s√©lections valid√©es"):
                        if not st.session_state.selected_data.empty:
                            csv_data = st.session_state.selected_data.to_csv(index=False)
                            st.download_button(
                                label="T√©l√©charger les donn√©es s√©lectionn√©es",
                                data=csv_data,
                                file_name=export_filename,
                                mime='text/csv'
                            )
                        else:
                            st.warning("Aucune s√©lection valid√©e √† exporter pour le moment.")
                else:
                    st.info("Aucune section valid√©e pour le moment.")
        
            # === Onglet Superposition ===
            with vis_tab[1]:
                st.header("Superposition des segments extraits (alignement √† t=0)")
                
                if "selected_data" in st.session_state and not st.session_state.selected_data.empty:
                    data_to_use = st.session_state.selected_data.copy()
                else:
                    st.warning("Aucune s√©lection d√©coup√©e n'est disponible. La superposition utilisera les donn√©es charg√©es.")
                    data_to_use = data.copy()  # Utilise le dataset complet
                
                # Choix des lots et de l'√©tape
                available_batches = sorted(data_to_use["Batch name"].unique())
                col1, col2 = st.columns(2)
                with col1:
                    selected_batches = st.multiselect(
                        "S√©lectionner les lots √† superposer",
                        options=available_batches,
                        default=available_batches[:2] if len(available_batches) >= 2 else []
                    )
                with col2:
                    steps_options = ["Toutes les √©tapes"] + sorted(data_to_use["Step"].unique())
                    overlay_step = st.selectbox("√âtape pour la superposition", options=steps_options, index=0)
                
                # Filtrer selon l'√©tape
                if overlay_step != "Toutes les √©tapes":
                    data_filtered = data_to_use[data_to_use["Step"] == overlay_step]
                else:
                    data_filtered = data_to_use.copy()
                
                # Choix du param√®tre √† superposer
                overlay_param = st.selectbox(
                    "Param√®tre √† superposer",
                    options=[col for col in data_filtered.columns if col not in ["Batch name", "Step", "Time", "Selection ID"]],
                    index=0,
                    key="overlay_param_superposition"
                )
                
                # Dans cette version, nous n'utilisons plus l'impuret√© pour d√©finir la couleur.
                # Nous utilisons simplement deux couleurs fixes que nous alternons.
                palette = ["skyblue", "purple"]
                
                if selected_batches:

                    impurity_columns = [col for col in lot_data.columns if "Impuret√©" in col]
                    
                    # Cr√©er une palette de couleurs quantitative pour les carr√©s de l√©gende
                    color_scale_a = px.colors.diverging.RdYlGn[::-1]  # Palette pour l'impuret√© A
                    color_scale_b = px.colors.diverging.RdYlGn[::-1]  # Palette pour l'impuret√© B
                    color_scale_c = px.colors.diverging.RdYlGn[::-1]  # Palette pour l'impuret√© C
                    
                    # Trouver les valeurs min et max d'impuret√© pour normaliser
                    all_impurities_a = [item for sublist in data.groupby("Batch name")["Impuret√© a"].apply(list).to_dict().values() for item in sublist]
                    all_impurities_b = [item for sublist in data.groupby("Batch name")["Impuret√© b"].apply(list).to_dict().values() for item in sublist]
                    all_impurities_c = [item for sublist in data.groupby("Batch name")["Impuret√© c"].apply(list).to_dict().values() for item in sublist]
                    
                    min_impurity_a, max_impurity_a = min(all_impurities_a), max(all_impurities_a)
                    min_impurity_b, max_impurity_b = min(all_impurities_b), max(all_impurities_b)
                    min_impurity_c, max_impurity_c = min(all_impurities_c), max(all_impurities_c)
                    
                    fig = go.Figure()

                    for i, batch in enumerate(selected_batches):
                        batch_data = data_filtered[data_filtered["Batch name"] == batch].copy()
                        if batch_data.empty:
                            continue
                        batch_data = batch_data.sort_values("Time").reset_index(drop=True)
                        batch_data["time_index"] = (batch_data["Time"] - batch_data["Time"].min()).dt.total_seconds()
                        
                        # Choix de la couleur en alternant dans la palette (pour les courbes)
                        color = palette[i % len(palette)]
                        
                        # R√©cup√©rer les valeurs d'impuret√© pour ce lot
                        batch_impurity_a = data.groupby("Batch name")["Impuret√© a"].apply(list).to_dict()[batch][0]  # Premi√®re valeur d'impuret√© A
                        batch_impurity_b = data.groupby("Batch name")["Impuret√© b"].apply(list).to_dict()[batch][0]  # Premi√®re valeur d'impuret√© B
                        batch_impurity_c = data.groupby("Batch name")["Impuret√© c"].apply(list).to_dict()[batch][0]  # Premi√®re valeur d'impuret√© C
                        
                        # Normaliser les valeurs d'impuret√©
                        normalized_impurity_a = (batch_impurity_a - min_impurity_a) / (max_impurity_a - min_impurity_a)
                        normalized_impurity_b = (batch_impurity_b - min_impurity_b) / (max_impurity_b - min_impurity_b)
                        normalized_impurity_c = (batch_impurity_c - min_impurity_c) / (max_impurity_c - min_impurity_c)
                        
                        # Mapper les valeurs normalis√©es √† des couleurs dans les palettes
                        color_a = color_scale_a[int(normalized_impurity_a * (len(color_scale_a) - 1))]
                        color_b = color_scale_b[int(normalized_impurity_b * (len(color_scale_b) - 1))]
                        color_c = color_scale_c[int(normalized_impurity_c * (len(color_scale_c) - 1))]
                        
                        # Ajouter la trace principale (courbe)
                        fig.add_trace(go.Scatter(
                            x=batch_data["time_index"],
                            y=batch_data[overlay_param],
                            mode="lines+markers",
                            name=f"{batch}",  # Nom du lot
                            line=dict(width=2, color=color),  # Couleur de la courbe (inchang√©e)
                            marker=dict(size=4, color=color),  # Marqueurs de la courbe (inchang√©s)
                            showlegend=True,  # Afficher cette trace dans la l√©gende
                        ))
        
                        # Ajouter des marqueurs pour les impuret√©s dans la l√©gende
                        fig.add_trace(go.Scatter(
                            x=[None],  # Pas de donn√©es sur l'axe X
                            y=[None],  # Pas de donn√©es sur l'axe Y
                            mode="markers",
                            name=f"Impuret√© a: {batch_impurity_a:.2f}",  # L√©gende pour l'impuret√© A
                            marker=dict(size=10, color=color_a, symbol="square"),  # Carr√© de couleur pour l'impuret√© A
                            showlegend=True,  # Afficher dans la l√©gende
                            legendgroup=batch,  # Grouper par lot
                        ))
                        
                        fig.add_trace(go.Scatter(
                            x=[None],
                            y=[None],
                            mode="markers",
                            name=f"Impuret√© b: {batch_impurity_b:.2f}",  # L√©gende pour l'impuret√© B
                            marker=dict(size=10, color=color_b, symbol="square"),  # Carr√© de couleur pour l'impuret√© B
                            showlegend=True,
                            legendgroup=batch,
                        ))
                        
                        fig.add_trace(go.Scatter(
                            x=[None],
                            y=[None],
                            mode="markers",
                            name=f"Impuret√© c: {batch_impurity_c:.2f}",  # L√©gende pour l'impuret√© C
                            marker=dict(size=10, color=color_c, symbol="square"),  # Carr√© de couleur pour l'impuret√© C
                            showlegend=True,
                            legendgroup=batch,
                        ))
                    
                    fig.update_layout(
                        title="Superposition des segments (alignement √† t=0)",
                        xaxis_title="Temps (secondes depuis le d√©but du segment)",
                        yaxis_title=overlay_param,
                        height=600,
                        showlegend=True  # Assurez-vous que la l√©gende est visible
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Exportation des courbes align√©es
                    aligned_df = {}
                    for batch in selected_batches:
                        batch_data = data_filtered[data_filtered["Batch name"] == batch].copy()
                        if batch_data.empty:
                            continue
                        batch_data = batch_data.sort_values("Time").reset_index(drop=True)
                        batch_data["time_index"] = (batch_data["Time"] - batch_data["Time"].min()).dt.total_seconds()
                        aligned_df[batch] = batch_data[[overlay_param, "time_index"]]
                    if aligned_df:
                        export_df = pd.concat(aligned_df, axis=1)
                        csv_data = export_df.to_csv(index=True)
                        st.download_button(
                            label="T√©l√©charger les courbes align√©es",
                            data=csv_data,
                            file_name=f"aligned_curves_{overlay_param}_{overlay_step}.csv",
                            mime='text/csv'
                        )
                else:
                    st.warning("S√©lectionnez au moins un lot pour la superposition.")


        # -------------------------------
        # Onglets pour Functional Boxplot
        # -------------------------------
        with vis_tabs[3]:
            vis_tab = st.tabs(["Comparaison Courbe moyenne", "Functional boxplot"])
            

            with vis_tab[0]:  # Assurez-vous que c'est le bon index pour votre onglet
                st.subheader("Comparaison avec la courbe moyenne")

                # V√©rification des colonnes n√©cessaires
                if 'Batch name' in data.columns and 'Time' in data.columns:
                    # Normalisation du temps pour chaque lot (0% √† 100%)
                    data_sorted = data.copy()
                    data_sorted['Relative Time'] = data.groupby('Batch name')['Time'].transform(
                        lambda x: (x - x.min()) / (x.max() - x.min()) * 100  # Normalisation en pourcentage
                    )

                    # Arrondir le temps normalis√© √† l'entier le plus proche (regroupement par 1%)
                    data_sorted['Relative Time'] = data_sorted['Relative Time'].round()

                    # S√©lection du param√®tre √† afficher
                    available_params = [col for col in data.columns if col not in ['Batch name', 'Step', 'Time']]
                    selected_param = st.selectbox("Param√®tre √† afficher", available_params, key="optimal_curve_param")

                    # Choix de la m√©trique pour la courbe optimale
                    metric_options = ["M√©diane","Moyenne", "Moyenne mobile (liss√©e)"]
                    selected_metric = st.selectbox("M√©trique pour la courbe optimale", metric_options, index=0, key="optimal_metric")  # Moyenne par d√©faut

                    # Calcul de la courbe optimale
                    if selected_metric == "Moyenne":
                        optimal_curve = data_sorted.groupby('Relative Time')[selected_param].mean().reset_index()
                    elif selected_metric == "M√©diane":
                        optimal_curve = data_sorted.groupby('Relative Time')[selected_param].median().reset_index()
                    elif selected_metric == "Moyenne mobile (liss√©e)":
                        window_size = st.slider(
                            "Taille de la fen√™tre pour la moyenne mobile",
                            min_value=3,
                            max_value=20,
                            value=5,
                            key="window_size"
                        )
                        optimal_curve = data_sorted.groupby('Relative Time')[selected_param].mean().rolling(window=window_size, min_periods=1).mean().reset_index()
                    
                    optimal_curve.rename(columns={selected_param: 'Courbe optimale'}, inplace=True)

                    # S√©lection des batchs √† afficher
                    selected_batches = st.multiselect(
                        "S√©lectionner les batchs √† afficher",
                        options=sorted(data['Batch name'].unique()),
                        default=sorted(data['Batch name'].unique())[:2],  # Par d√©faut, afficher 2 batchs
                        key="selected_batches"
                    )

                    # Filtrer les donn√©es pour les batchs s√©lectionn√©s
                    filtered_data = data_sorted[data_sorted['Batch name'].isin(selected_batches)]

                    # Cr√©ation du graphique avec Plotly
                    fig = go.Figure()

                    # Ajouter la courbe optimale
                    fig.add_trace(go.Scatter(
                        x=optimal_curve['Relative Time'],
                        y=optimal_curve['Courbe optimale'],
                        mode='lines',
                        name=f'Courbe optimale ({selected_metric})',
                        line=dict(color='#4c72b0', width=1)
                    ))

                    # Ajouter les courbes des batchs s√©lectionn√©s
                    for batch in selected_batches:
                        batch_data = filtered_data[filtered_data['Batch name'] == batch]
                        # Regrouper les donn√©es du batch par intervalle de 1%
                        batch_data_grouped = batch_data.groupby('Relative Time')[selected_param].median().reset_index()
                        fig.add_trace(go.Scatter(
                            x=batch_data_grouped['Relative Time'],
                            y=batch_data_grouped[selected_param],
                            mode='lines',
                            name=batch,
                            opacity=0.7  # Transparence pour mieux voir la superposition
                        ))

                    # Personnalisation du graphique
                    fig.update_layout(
                        title=f"Comparaison des batchs avec la courbe optimale ({selected_metric}) - {selected_param}",
                        xaxis_title="Progression du temps (%)",
                        yaxis_title=selected_param,
                        legend_title="Batchs",
                        showlegend=True
                    )

                    # Affichage du graphique
                    st.plotly_chart(fig, use_container_width=True)

                    # Option pour t√©l√©charger les donn√©es de la courbe optimale
                    csv = optimal_curve.to_csv(index=False)
                    st.download_button(
                        label="T√©l√©charger la courbe optimale",
                        data=csv,
                        file_name=f"courbe_optimale_{selected_param}_{selected_metric}.csv",
                        mime='text/csv',
                    )
                else:
                    st.error("Les colonnes n√©cessaires ('Batch name', 'Time') sont manquantes dans les donn√©es.")

            with vis_tab[1]: # Nouvel onglet pour les statistiques d√©taill√©es
                st.subheader("Functional boxplot")
                st.markdown("Statistiques d√©taill√©es avec m√©diane, quartiles et min/max")

                if 'Batch name' in data.columns and 'Time' in data.columns:
                    data_sorted = data.copy()
                    data_sorted['Relative Time'] = data.groupby('Batch name')['Time'].transform(
                        lambda x: (x - x.min()) / (x.max() - x.min()) * 100
                    )
                    data_sorted['Relative Time'] = data_sorted['Relative Time'].round()

                    available_params = [col for col in data.columns if col not in ['Batch name', 'Step', 'Time']]
                    selected_param = st.selectbox("Param√®tre √† afficher", available_params, key="stats_param")

                    stats = data_sorted.groupby('Relative Time')[selected_param].agg(
                        ['median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75), 'min', 'max']
                    ).reset_index()
                    stats.columns = ['Relative Time', 'M√©diane', '25e percentile', '75e percentile', 'Min', 'Max']

                    selected_batches = st.multiselect(
                        "S√©lectionner les batchs √† afficher",
                        options=sorted(data['Batch name'].unique()),
                        default=sorted(data['Batch name'].unique())[:1],
                        key="selected_batches_stats"
                    )

                    filtered_data = data_sorted[data_sorted['Batch name'].isin(selected_batches)]

                    fig = go.Figure()

                    # Plage Min ‚Üí 25e percentile (fond bleu clair)
                    fig.add_trace(go.Scatter(
                        x=stats['Relative Time'], y=stats['Min'],
                        mode='lines', line=dict(color='rgba(0, 0, 0, 0)'),
                        fill=None, showlegend=False
                    ))
                    fig.add_trace(go.Scatter(
                        x=stats['Relative Time'], y=stats['25e percentile'],
                        mode='lines', line=dict(color='rgba(0, 0, 0, 0)'),
                        fill='tonexty', fillcolor='rgba(76, 114, 176, 0.1)',  # Bleu clair
                        name='Plage Min - 25e percentile'
                    ))

                    # Plage 25e percentile ‚Üí 75e percentile (interquartile)
                    fig.add_trace(go.Scatter(
                        x=stats['Relative Time'], y=stats['75e percentile'],
                        mode='lines', line=dict(color='rgba(0, 0, 0, 0)'),
                        fill='tonexty', fillcolor='rgba(76, 114, 176, 0.3)',  # Bleu plus fonc√©
                        name='Zone interquartile (25e-75e percentile)'
                    ))

                    # Plage 75e percentile ‚Üí Max (fond bleu clair)
                    fig.add_trace(go.Scatter(
                        x=stats['Relative Time'], y=stats['Max'],
                        mode='lines', line=dict(color='rgba(0, 0, 0, 0)'),
                        fill='tonexty', fillcolor='rgba(76, 114, 176, 0.1)',  # Bleu clair
                        name='Plage 75e percentile - Max'
                    ))

                    # M√©diane plus fine
                    fig.add_trace(go.Scatter(
                        x=stats['Relative Time'], y=stats['M√©diane'],
                        mode='lines', name='M√©diane',
                        line=dict(color='#4c72b0', width=1)
                    ))

                    # Batchs plus √©pais et couleurs vari√©es sans bleu
                    non_blue_colors = ['#e377c2', '#8c564b', '#ff7f0e', '#2ca02c', '#9467bd', '#d62728']
                    for i, batch in enumerate(selected_batches):
                        batch_data = filtered_data[filtered_data['Batch name'] == batch]
                        batch_data_grouped = batch_data.groupby('Relative Time')[selected_param].median().reset_index()
                        fig.add_trace(go.Scatter(
                            x=batch_data_grouped['Relative Time'], y=batch_data_grouped[selected_param],
                            mode='lines', name=batch,
                            line=dict(color=non_blue_colors[i % len(non_blue_colors)], width=3)  # Plus √©pais
                        ))

                    fig.update_layout(
                        title=f"Statistiques d√©taill√©es pour {selected_param}",
                        xaxis_title="Progression du temps (%)",
                        yaxis_title=selected_param,
                        legend_title="L√©gende",
                        showlegend=True,
                        plot_bgcolor='white',
                        xaxis=dict(showgrid=True, gridcolor='lightgray'),
                        yaxis=dict(showgrid=True, gridcolor='lightgray')
                    )

                    st.plotly_chart(fig, use_container_width=True)

                    csv = stats.to_csv(index=False)
                    st.download_button(
                        label="T√©l√©charger les statistiques",
                        data=csv,
                        file_name=f"statistiques_{selected_param}.csv",
                        mime='text/csv',
                    )
                else:
                    st.error("Les colonnes n√©cessaires ('Batch name', 'Time') sont manquantes dans les donn√©es.")


    # -----------------------------------
    # Onglet 2 : Analyse Statistique
    # -----------------------------------
    with main_tabs[1]:
        st.header("Analyse Statistique")
        stat_tabs = st.tabs(["Analyse des Tendances", "Analyse des Corr√©lations"])
        
        # --- Sous-onglet : Analyse des Tendances ---
        with stat_tabs[0]:
            st.subheader("Analyse des Tendances")
            trend_param = st.selectbox("Param√®tre √† analyser", options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']], key="trend_param")
            trend_step = st.selectbox("√âtape", options=["Toutes les √©tapes"] + sorted(data['Step'].unique()), key="trend_step")
            trend_data = data if trend_step == "Toutes les √©tapes" else data[data['Step'] == trend_step]
            batch_stats = trend_data.groupby('Batch name')[trend_param].agg(['mean', 'std', 'min', 'max']).reset_index()
            st.subheader("Statistiques par Lot")
            st.dataframe(batch_stats)
            st.subheader(f"Moyenne de {trend_param} par lot")
            st.bar_chart(batch_stats.set_index('Batch name')[['mean']])
            if 'Time' in trend_data.columns:
                time_trend = trend_data.sort_values('Time')
                st.subheader(f"Analyse de la tendance globale de {trend_param}")
                granularity_options = {
                    "5 minutes": 300,
                    "30 minutes": 1800,
                    "1 heure": 3600,
                    "6 heures": 21600,
                    "12 heures": 43200,
                    "1 jour": 86400,
                    "1 semaine": 604800
                }
                granularity_name = st.selectbox(
                    "Choisir l'intervalle d'agr√©gation",
                    options=list(granularity_options.keys()),
                    index=2  # 1 heure par d√©faut
                )
                granularity_seconds = granularity_options[granularity_name]
                min_date = time_trend['Time'].min().date()
                max_date = time_trend['Time'].max().date()

                date_range = st.date_input(
                    "S√©lectionner une plage de dates",
                    [min_date, max_date],
                    min_value=min_date,
                    max_value=max_date
                )
                if len(date_range) == 2:
                    start_date, end_date = date_range
                    time_trend = time_trend[(time_trend['Time'].dt.date >= start_date) & (time_trend['Time'].dt.date <= end_date)]
                    
                # V√©rifier qu'il y a suffisamment de donn√©es
                if len(time_trend) < 3:
                    st.warning("Pas assez de donn√©es pour analyser la tendance. Veuillez s√©lectionner une plage de dates plus large.")
                else:
                    # Cr√©er des groupes temporels bas√©s sur l'intervalle en secondes
                    time_trend['timestamp_s'] = time_trend['Time'].astype('int64') // 1e9
                    time_trend['time_group'] = (time_trend['timestamp_s'] // granularity_seconds) * granularity_seconds
                    time_trend['time_group_readable'] = pd.to_datetime(time_trend['time_group'], unit='s')
                    
                    # Calculer la moyenne pour chaque groupe temporel
                    grouped_means = time_trend.groupby('time_group_readable')[trend_param].mean().reset_index()
                    
                    # Si le nombre de points est trop √©lev√©, √©chantillonner
                    if len(grouped_means) > 100:
                        st.info(f"Les donn√©es sont √©chantillonn√©es pour am√©liorer la lisibilit√© (plus de {len(grouped_means)} points)")
                        sample_step = len(grouped_means) // 100 + 1
                        grouped_means = grouped_means.iloc[::sample_step].copy()
                    
                    # Cr√©er le graphique avec Plotly
                    fig = go.Figure()
                    
                    # Ajouter la ligne de tendance moyenne
                    fig.add_trace(go.Scatter(
                        x=grouped_means['time_group_readable'],
                        y=grouped_means[trend_param],
                        mode='lines+markers',
                        line=dict(color='rgba(0, 100, 200, 1)', width=3),
                        marker=dict(size=8),
                        name='Moyenne'
                    ))
                    
                    # Mise en forme du graphique
                    fig.update_layout(
                        title=f"Tendance moyenne de {trend_param} (intervalle: {granularity_name})",
                        xaxis_title="P√©riode",
                        yaxis_title=trend_param,
                        hovermode="closest",
                        plot_bgcolor='rgba(240,248,255,0.95)',  # Fond l√©g√®rement bleut√©
                        height=500
                    )
                    
                    # Ajouter une grille l√©g√®re pour faciliter la lecture
                    date_format = '%d/%m/%Y %H:%M' if granularity_seconds < 86400 else '%d/%m/%Y'
                    fig.update_xaxes(
                        tickformat=date_format,
                        tickangle=45,
                        showgrid=True,
                        gridwidth=1,
                        gridcolor='rgba(200,200,200,0.3)'
                    )
                    
                    fig.update_yaxes(
                        showgrid=True,
                        gridwidth=1,
                        gridcolor='rgba(200,200,200,0.3)'
                    )
                    
                    # Afficher le graphique
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Option pour t√©l√©charger les donn√©es de la tendance
                    with st.expander("Voir les donn√©es de tendance"):
                        # Ajouter une colonne format√©e pour l'affichage
                        display_means = grouped_means.copy()
                        display_means['Date format√©e'] = display_means['time_group_readable'].dt.strftime(date_format)
                        display_means = display_means[['Date format√©e', trend_param]]
                        st.dataframe(display_means)
                        
                        # Option pour t√©l√©charger les statistiques
                        csv = grouped_means.to_csv(index=False)
                        st.download_button(
                            label="T√©l√©charger les donn√©es de tendance",
                            data=csv,
                            file_name=f"tendance_{trend_param}_{granularity_name.replace(' ', '_')}.csv",
                            mime='text/csv',
                        )
                        
            else:
                st.warning("Donn√©es temporelles non disponibles pour l'analyse.")

            st.subheader("D√©tection d'Anomalies par ACP Fonctionnelle")
            st.markdown("""
            Cette m√©thode utilise l'analyse en composantes principales fonctionnelle pour d√©tecter les anomalies 
            dans les courbes temporelles. Elle est particuli√®rement adapt√©e aux donn√©es de proc√©d√©s industriels 
            qui sont de nature fonctionnelle (√©volution temporelle).
            """)
            def functional_pca_anomaly_detection(data, param, n_components=2, threshold_factor=2.0):
                """
                D√©tecte les anomalies en utilisant une ACP fonctionnelle simplifi√©e
                
                Args:
                    data: DataFrame contenant les donn√©es
                    param: Nom du param√®tre √† analyser
                    n_components: Nombre de composantes principales √† utiliser
                    threshold_factor: Facteur multiplicatif pour le seuil de d√©tection
                    
                Returns:
                    Tuple contenant:
                    - DataFrame avec les anomalies d√©tect√©es et leur score
                    - Array X des courbes originales
                    - Array X_scaled des courbes normalis√©es
                    - Array X_reconstructed des courbes reconstruites
                    - Array des indices temporels normalis√©s
                    - Liste des noms de lots
                    - Objet PCA
                """
                # V√©rifier que le DataFrame n'est pas vide
                if data.empty:
                    return pd.DataFrame(), None, None, None, None, None, None
                
                # Extraire le param√®tre √† analyser
                if param not in data.columns:
                    return pd.DataFrame(), None, None, None, None, None, None
                
                # Standardiser les donn√©es (centrage et r√©duction)
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                
                # Pivoter le DataFrame pour avoir une ligne par lot et une colonne par point temporel
                # D'abord, cr√©er un indice temporel normalis√©
                lots = data['Batch name'].unique()
                results = []
                
                # Cr√©er une liste pour stocker les courbes normalis√©es
                aligned_curves = []
                lot_names = []
                
                for batch in lots:
                    batch_data = data[data['Batch name'] == batch].copy()
                    if len(batch_data) < 5:  # Ignorer les lots avec trop peu de points
                        continue
                        
                    # Trier par ordre temporel si disponible
                    if 'Time' in batch_data.columns:
                        batch_data = batch_data.sort_values('Time').reset_index(drop=True)
                    
                    # Extraire le param√®tre et cr√©er une s√©rie temporelle normalis√©e en temps
                    # (0 √† 100% de la dur√©e du lot)
                    values = batch_data[param].values
                    # Normaliser √† une longueur fixe (100 points) par interpolation lin√©aire
                    from scipy.interpolate import interp1d
                    old_indices = np.linspace(0, 1, len(values))
                    new_indices = np.linspace(0, 1, 100)  # 100 points pour toutes les courbes
                    
                    # G√©rer les NaN en les rempla√ßant par des interpolations
                    mask = ~np.isnan(values)
                    if sum(mask) > 1:  # Au moins 2 points valides pour l'interpolation
                        f = interp1d(old_indices[mask], values[mask], bounds_error=False, fill_value="extrapolate")
                        normalized_values = f(new_indices)
                        aligned_curves.append(normalized_values)
                        lot_names.append(batch)
                
                if len(aligned_curves) < 3:
                    st.warning("Pas assez de lots pour effectuer une ACP fonctionnelle (minimum 3 requis).")
                    return pd.DataFrame(), None, None, None, None, None, None
                
                # Convertir en array numpy pour l'ACP
                X = np.array(aligned_curves)
                
                # Standardiser les donn√©es
                X_scaled = scaler.fit_transform(X)
                
                # Appliquer l'ACP
                from sklearn.decomposition import PCA
                pca = PCA(n_components=n_components)
                pca.fit(X_scaled)
                
                # Projeter les donn√©es sur les composantes principales
                X_pca = pca.transform(X_scaled)
                
                # Reconstruire les donn√©es √† partir des composantes principales
                X_reconstructed = pca.inverse_transform(X_pca)
                
                # Calculer l'erreur de reconstruction pour chaque lot
                reconstruction_errors = np.mean((X_scaled - X_reconstructed) ** 2, axis=1)
                
                # Calculer le seuil pour d√©tecter les anomalies
                threshold = np.mean(reconstruction_errors) + threshold_factor * np.std(reconstruction_errors)
                
                # D√©terminer les anomalies
                anomalies_mask = reconstruction_errors > threshold
                
                # Cr√©er un DataFrame avec les r√©sultats
                results_df = pd.DataFrame({
                    'Batch name': lot_names,
                    'Reconstruction Error': reconstruction_errors,
                    'Is Anomaly': anomalies_mask,
                    'Threshold': threshold
                })
                
                # Afficher le pourcentage de variance expliqu√©e
                explained_variance = np.sum(pca.explained_variance_ratio_) * 100
                st.info(f"Les {n_components} premi√®res composantes expliquent {explained_variance:.2f}% de la variance totale.")
                
                return results_df, X, X_scaled, X_reconstructed, new_indices, lot_names, pca
            
            # Options d'analyse
            n_components = st.slider(
                "Nombre de composantes principales",
                min_value=1,
                max_value=10,
                value=3,
                key="fpca_n_components"
            )
            
            threshold_factor = st.slider(
                "Facteur multiplicatif pour le seuil d'anomalie",
                min_value=0.5,
                max_value=5.0,
                value=2.0,
                step=0.1,
                key="fpca_threshold"
            )
            
            # Effectuer la d√©tection d'anomalies
            try:
                results_df, X, X_scaled, X_reconstructed, time_indices, lot_names, pca = functional_pca_anomaly_detection(
                    trend_data, trend_param, n_components, threshold_factor
                )
            
                if not results_df.empty and pca is not None:  # V√©rifier que pca est d√©fini
                    # Afficher les r√©sultats
                    st.subheader("R√©sultats de la d√©tection d'anomalies")
                    
                    # Cr√©er une visualisation des erreurs de reconstruction
                    fig_error = go.Figure()
                    
                    # Ajouter les erreurs de reconstruction
                    fig_error.add_trace(go.Bar(
                        x=results_df['Batch name'],
                        y=results_df['Reconstruction Error'],
                        name="Erreur de reconstruction",
                        marker_color=['red' if anomaly else 'blue' for anomaly in results_df['Is Anomaly']]
                    ))
                    
                    # Ajouter la ligne de seuil
                    fig_error.add_trace(go.Scatter(
                        x=results_df['Batch name'],
                        y=[results_df['Threshold'].iloc[0]] * len(results_df),
                        mode='lines',
                        name="Seuil",
                        line=dict(color='red', dash='dash', width=2)
                    ))
                    
                    # Mise en forme
                    fig_error.update_layout(
                        title=f"Erreurs de reconstruction pour {trend_param}",
                        xaxis_title="Lot",
                        yaxis_title="Erreur de reconstruction",
                        showlegend=True
                    )
                    
                    st.plotly_chart(fig_error, use_container_width=True)
                    
                    # Afficher les d√©tails des anomalies
                    anomalies = results_df[results_df['Is Anomaly']]
                    st.subheader(f"Lots anormaux d√©tect√©s: {len(anomalies)}/{len(results_df)}")
                    
                    if not anomalies.empty:
                        st.dataframe(anomalies)
                        
                        # Visualiser les courbes des lots anormaux vs la moyenne
                        fig_curves = go.Figure()
                        
                        # Calculer la courbe moyenne (des lots non-anormaux)
                        normal_indices = ~results_df['Is Anomaly'].values
                        if np.any(normal_indices):
                            mean_curve = np.mean(X_scaled[normal_indices], axis=0)
                            
                            # Ajouter la courbe moyenne
                            fig_curves.add_trace(go.Scatter(
                                x=np.linspace(0, 100, len(mean_curve)),
                                y=mean_curve,
                                mode='lines',
                                name='Courbe moyenne normale',
                                line=dict(color='blue', width=3)
                            ))
                            
                            # Ajouter les courbes anormales
                            for i, is_anomaly in enumerate(results_df['Is Anomaly']):
                                if is_anomaly:
                                    fig_curves.add_trace(go.Scatter(
                                        x=np.linspace(0, 100, len(X_scaled[i])),
                                        y=X_scaled[i],
                                        mode='lines',
                                        name=f"Anomalie: {results_df['Batch name'].iloc[i]}",
                                        line=dict(color='red')
                                    ))
                            
                            # Mise en forme
                            fig_curves.update_layout(
                                title=f"Comparaison des courbes anormales vs moyenne des courbes normales",
                                xaxis_title="Progression du lot (%)",
                                yaxis_title=f"{trend_param} (normalis√©)",
                                showlegend=True
                            )
                            
                            st.plotly_chart(fig_curves, use_container_width=True)
                        else:
                            st.info("Tous les lots sont consid√©r√©s comme anormaux. Ajustez le seuil d'anomalie.")
                        
                        # Visualisation des composantes principales
                        fig_pca = go.Figure()
                        
                        # Visualiser les deux premi√®res composantes principales
                        for i, (batch, is_anomaly) in enumerate(zip(results_df['Batch name'], results_df['Is Anomaly'])):
                            color = 'red' if is_anomaly else 'blue'
                            fig_pca.add_trace(go.Scatter(
                                x=[pca.components_[0, j] for j in range(len(pca.components_[0]))],
                                y=[X_scaled[i, j] for j in range(len(X_scaled[i]))],
                                mode='markers',
                                name=batch,
                                marker=dict(color=color, size=5),
                                visible="legendonly"  # Masquer par d√©faut pour √©viter la surcharge
                            ))
                        
                        # Ajouter la premi√®re composante principale
                        weights = pca.components_[0]
                        fig_pca.add_trace(go.Scatter(
                            x=np.linspace(0, 100, len(weights)),
                            y=weights,
                            mode='lines',
                            name='1√®re composante principale',
                            line=dict(color='black', width=2)
                        ))
                        
                        # Si on a au moins 2 composantes
                        if n_components >= 2:
                            weights2 = pca.components_[1]
                            fig_pca.add_trace(go.Scatter(
                                x=np.linspace(0, 100, len(weights2)),
                                y=weights2,
                                mode='lines',
                                name='2√®me composante principale',
                                line=dict(color='purple', width=2, dash='dash')
                            ))
                        
                        # Mise en forme
                        fig_pca.update_layout(
                            title="Composantes principales fonctionnelles",
                            xaxis_title="Progression du lot (%)",
                            yaxis_title="Poids",
                            showlegend=True
                        )
                        
                        st.plotly_chart(fig_pca, use_container_width=True)
                        
                        # Option pour t√©l√©charger les r√©sultats
                        csv = anomalies.to_csv(index=False)
                        st.download_button(
                            label="T√©l√©charger la liste des anomalies",
                            data=csv,
                            file_name=f"anomalies_fpca_{trend_param}_{trend_step}.csv",
                            mime='text/csv',
                        )
                    else:
                        st.success(f"Aucune anomalie d√©tect√©e pour {trend_param} avec les param√®tres actuels.")

                    # Maintenant, r√©cup√©rons les scores PCA et faisons la matrice de corr√©lation
                    X_pca = pca.transform(X_scaled)
                    pca_df = pd.DataFrame(X_pca, columns=[f"PC{i+1}" for i in range(X_pca.shape[1])])
                    pca_df["Batch name"] = lot_names
                    
                    # R√©cup√©rer les impuret√©s : ici on suppose que chaque lot a une seule valeur par type
                    impurity_columns = ["Impuret√© a", "Impuret√© b", "Impuret√© c"]
                    impurity_df = data.groupby("Batch name")[impurity_columns].first().reset_index()
                    
                    # Fusionner les scores PCA avec les impuret√©s
                    merged_df = pd.merge(pca_df, impurity_df, on="Batch name", how="inner")
                    cols_corr = [f"PC{i+1}" for i in range(X_pca.shape[1])] + impurity_columns
                    corr_matrix = merged_df[cols_corr].corr()
                    
                    st.subheader("Matrice de corr√©lation entre les composantes principales et les impuret√©s")
                    fig = px.imshow(
                        corr_matrix,
                        text_auto='.4f',                # pour afficher la valeur de corr√©lation dans chaque case
                        color_continuous_scale='RdBu_r',  # palette de couleurs allant du rouge au bleu
                        range_color=[-1, 1]            # l‚Äô√©chelle de couleurs de -1 √† 1
                    )
        
                    # Augmenter la taille du texte dans chaque case
                    fig.update_traces(textfont_size=8)
        
                    fig.update_layout(
                        xaxis=dict(side="bottom")  # pour avoir l‚Äôaxe X en bas (optionnel)
                    )
        
                    st.plotly_chart(fig, use_container_width=True)

                else:
                    st.warning("Impossible d'effectuer l'analyse. V√©rifiez les donn√©es s√©lectionn√©es.")
            except Exception as e:
                st.error(f"Une erreur s'est produite lors de l'analyse: {e}")
                import traceback
                st.code(traceback.format_exc())



        
        # --- Sous-onglet : Analyse des Corr√©lations ---
        with stat_tabs[1]:
            st.subheader("Analyse des Corr√©lations")
            st.markdown("""
            Cette section permet d'analyser les corr√©lations entre les diff√©rents param√®tres
            et d'identifier les relations importantes.
            """)
            
            # S√©lection des param√®tres pour l'analyse de corr√©lation
            corr_params = st.multiselect(
                "Param√®tres pour l'analyse de corr√©lation",
                options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
                default=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
                key="corr_params"
            )
            
            if corr_params:
                # Filtrer les donn√©es
                corr_data = data[corr_params].dropna()
                
                # Calculer la matrice de corr√©lation
                corr_matrix = corr_data.corr()
                
                # Afficher la matrice de corr√©lation comme un tableau
                st.subheader("Matrice de Corr√©lation entre les Param√®tres")
                st.dataframe(corr_matrix)
                
                # Analyse des relations entre variables
                st.subheader("Relations entre Variables")
                
                col1, col2 = st.columns(2)
                with col1:
                    x_var = st.selectbox("Variable X", options=corr_params, index=0, key="x_var")
                with col2:
                    y_var = st.selectbox("Variable Y", options=[p for p in corr_params if p != x_var], index=0, key="y_var")
                
                # Pr√©parer les donn√©es pour le scatter plot
                scatter_data = data[[x_var, y_var]].dropna()
                
                # Afficher le scatter plot
                st.subheader(f"Relation entre {x_var} et {y_var}")
                
                # Cr√©er un dataframe temporaire pour le scatter plot
                scatter_df = pd.DataFrame({
                    x_var: scatter_data[x_var],
                    y_var: scatter_data[y_var]
                })
                
                # Utiliser st.scatter_chart qui est disponible dans les versions r√©centes de Streamlit
                # Si ce n'est pas disponible, fallback vers une alternative
                try:
                    st.scatter_chart(scatter_df, x=x_var, y=y_var)
                except:
                    st.write("Nuage de points:")
                    st.dataframe(scatter_df.head(100))
                    st.info("Aper√ßu limit√© aux 100 premiers points. Pour une visualisation compl√®te, t√©l√©chargez les donn√©es.")
                
                # Option d'analyse par groupe simplifi√©e
                color_var = st.selectbox(
                    "Grouper par",
                    options=["Aucun groupement"] + ["Batch name", "Step"],
                    index=0,
                    key="color_var"
                )
                
                if color_var != "Aucun groupement":
                    # Calculer des statistiques par groupe
                    grouped_stats = data.groupby(color_var)[[x_var, y_var]].agg(['mean', 'std']).reset_index()
                    
                    st.subheader(f"Statistiques par {color_var}")
                    st.dataframe(grouped_stats)
                    
                    # Afficher les statistiques sous forme de tableau
                    summary_stats = data.groupby(color_var)[y_var].agg(['count', 'mean', 'std', 'min', 'max']).reset_index()
                    
                    st.subheader(f"Distribution de {y_var} par {color_var}")
                    st.dataframe(summary_stats)
            else:
                st.warning("Veuillez s√©lectionner au moins un param√®tre pour l'analyse.")
    
    # -----------------------------------
    # Onglet 3 : Pr√©diction
    # -----------------------------------
    with main_tabs[2]:
        st.header("Pr√©diction")
        # Cr√©ation de l'onglet de pr√©diction
        st.subheader("Pr√©diction des Param√®tres")
        
        # S√©lection des param√®tres
        col1, col2, col3 = st.columns(3)
        with col1:
            pred_param = st.selectbox(
                "Param√®tre √† pr√©dire",
                options=[
                    "Temp√©rature fond de cuve", 
                    "Temp√©rature haut de colonne", 
                    "Temp√©rature r√©acteur"
                ] + [col for col in data.columns if col not in ['Batch name', 'Step', 'Time']],
                key="pred_param"
            )
        with col2:
            pred_batch = st.selectbox(
                "Lot √† analyser",
                options=sorted(data['Batch name'].unique()),
                key="pred_batch"
            )
        with col3:
            pred_step = st.selectbox(
                "√âtape √† analyser",
                options=["Toutes les √©tapes"] + sorted(data['Step'].unique()),
                key="pred_step"
            )
        
        # Filtrer les donn√©es pour le lot et l'√©tape s√©lectionn√©s
        if pred_step == "Toutes les √©tapes":
            pred_data = data[data['Batch name'] == pred_batch]
        else:
            pred_data = data[(data['Batch name'] == pred_batch) & (data['Step'] == pred_step)]
        
        # S√©lection du mod√®le de r√©gression
        model_type = st.selectbox(
            "Mod√®le de r√©gression",
            options=["R√©gression lin√©aire multiple", "XGBoost", "XGBoost avec FATS"],
            key="model_type"
        )
        
        # D√©finition des fonctions pour l'extraction de caract√©ristiques temporelles (FATS)
        def extract_time_series_features(data):
            """
            Extraire des caract√©ristiques statistiques d'une s√©rie temporelle
            """
            features = {}
            
            # Statistiques de base
            features['mean'] = np.mean(data)
            features['std'] = np.std(data)
            features['min'] = np.min(data)
            features['max'] = np.max(data)
            
            # Quartiles
            features['q25'] = np.percentile(data, 25)
            features['median'] = np.median(data)
            features['q75'] = np.percentile(data, 75)
            features['iqr'] = features['q75'] - features['q25']  # √âcart interquartile
            
            # Forme de la distribution
            features['skewness'] = 0
            features['kurtosis'] = 0
            if len(data) > 3:  # Besoin d'au moins 4 points pour ces statistiques
                from scipy import stats
                features['skewness'] = stats.skew(data)
                features['kurtosis'] = stats.kurtosis(data)
            
            # Tendance
            if len(data) > 1:
                x = np.arange(len(data))
                from scipy import stats
                slope, _, r_value, _, _ = stats.linregress(x, data)
                features['slope'] = slope
                features['r_squared'] = r_value**2
            else:
                features['slope'] = 0
                features['r_squared'] = 0
            
            # Autocorr√©lation (lag 1)
            if len(data) > 1:
                features['autocorr_1'] = np.corrcoef(data[:-1], data[1:])[0, 1] if len(data) > 1 else 0
            else:
                features['autocorr_1'] = 0
            
            # Caract√©ristiques suppl√©mentaires si vous avez plus de donn√©es
            if len(data) > 4:
                # Taux de changement
                diff = np.diff(data)
                features['mean_change'] = np.mean(np.abs(diff))
                features['max_change'] = np.max(np.abs(diff))
                
                # Fr√©quence des changements de direction
                direction_changes = np.sum(diff[1:] * diff[:-1] < 0)
                features['direction_changes'] = direction_changes / (len(data) - 2) if len(data) > 2 else 0
            else:
                features['mean_change'] = 0
                features['max_change'] = 0
                features['direction_changes'] = 0
            
            return features
        
        def prepare_features_for_xgboost(data, target_param, feature_params, step=None):
            """
            Pr√©parer un DataFrame avec des caract√©ristiques extraites pour XGBoost
            """
            if step and step != "Toutes les √©tapes":
                data = data[data['Step'] == step]
            
            lots = data['Batch name'].unique()
            features_data = []
            
            for batch in lots:
                batch_data = data[data['Batch name'] == batch]
                
                # V√©rifier si le lot a des donn√©es pour le param√®tre cible
                if target_param in batch_data.columns and not batch_data[target_param].isnull().all():
                    # Extraire la valeur cible (par exemple, la moyenne ou la derni√®re valeur)
                    target_value = batch_data[target_param].mean()  # ou .iloc[-1] pour la derni√®re valeur
                    
                    # Extraire les caract√©ristiques pour chaque param√®tre d'entr√©e
                    batch_features = {'Batch name': batch, 'target': target_value}
                    
                    for param in feature_params:
                        if param in batch_data.columns and not batch_data[param].isnull().all():
                            param_values = batch_data[param].dropna().values
                            param_features = extract_time_series_features(param_values)
                            
                            # Pr√©fixer les noms des caract√©ristiques avec le nom du param√®tre
                            for feature_name, feature_value in param_features.items():
                                batch_features[f"{param}_{feature_name}"] = feature_value
                    
                    features_data.append(batch_features)
            
            # Cr√©er un DataFrame √† partir des caract√©ristiques extraites
            features_df = pd.DataFrame(features_data)
            return features_df
        
        # Traitement selon le type de mod√®le
        if not pred_data.empty:
            pred_data_reset = pred_data.reset_index(drop=True)
            
            # Approche standard pour la r√©gression lin√©aire multiple et XGBoost simple
            if model_type in ["R√©gression lin√©aire multiple", "XGBoost"]:
                st.markdown("""
                Ce mod√®le utilise les relations entre diff√©rents param√®tres pour pr√©dire le param√®tre cible.
                """)
                
                # S√©lection des variables explicatives
                feature_vars = st.multiselect(
                    "Variables explicatives",
                    options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time', pred_param]],
                    default=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time', pred_param]][:2],
                    key="feature_vars"
                )
                
                if feature_vars and len(feature_vars) > 0:
                    # Pr√©paration des donn√©es
                    X = pred_data_reset[feature_vars].values
                    y = pred_data_reset[pred_param].values
                    
                    # V√©rifier s'il y a des valeurs manquantes
                    valid_mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
                    X_valid = X[valid_mask]
                    y_valid = y[valid_mask]
                    
                    if len(X_valid) > 0:
                        # Calculer le point de s√©paration
                        train_pct = st.slider(
                            "Pourcentage de donn√©es pour l'entra√Ænement",
                            min_value=50,
                            max_value=90,
                            value=70,
                            step=5,
                            key="train_pct"
                        )
                        
                        split_idx = int(len(X_valid) * train_pct / 100)
                        X_train, X_test = X_valid[:split_idx], X_valid[split_idx:]
                        y_train, y_test = y_valid[:split_idx], y_valid[split_idx:]
                        
                        # V√©rification pour √©viter les erreurs
                        if X_train.shape[0] > 0 and X_test.shape[0] > 0:
                            try:
                                if model_type == "R√©gression lin√©aire multiple":
                                    # Ajuster un mod√®le de r√©gression lin√©aire multiple
                                    X_train_with_const = np.column_stack((np.ones(X_train.shape[0]), X_train))
                                    coeffs, residuals, rank, s = np.linalg.lstsq(X_train_with_const, y_train, rcond=None)
                                    
                                    # Faire des pr√©dictions
                                    X_test_with_const = np.column_stack((np.ones(X_test.shape[0]), X_test))
                                    y_pred = X_test_with_const @ coeffs
                                    
                                    # Afficher l'√©quation du mod√®le
                                    equation = f"{pred_param} = {coeffs[0]:.4f}"
                                    for i, feature in enumerate(feature_vars):
                                        equation += f" + {coeffs[i+1]:.4f} √ó {feature}"
                                    
                                    st.markdown(f"**√âquation du mod√®le:**")
                                    st.markdown(f"`{equation}`")
                                    
                                    # Afficher l'importance des variables
                                    importance = np.abs(coeffs[1:])
                                    importance_norm = importance / np.sum(importance)
                                    importance_df = pd.DataFrame({
                                        'Variable': feature_vars,
                                        'Coefficient': coeffs[1:],
                                        'Importance': importance_norm
                                    }).sort_values(by='Importance', ascending=False)
                                    
                                    st.subheader("Importance des Variables")
                                    st.dataframe(importance_df)
                                    
                                    # Version graphique de l'importance des variables
                                    st.bar_chart(importance_df.set_index('Variable')['Importance'])
                                    
                                elif model_type == "XGBoost":
                                    try:
                                        import xgboost as xgb
                                        # Cr√©er et entra√Æner le mod√®le XGBoost
                                        model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)
                                        model.fit(X_train, y_train)
                                        y_pred = model.predict(X_test)
                                        
                                    except ImportError:
                                        st.error("La biblioth√®que XGBoost n'est pas disponible. Veuillez installer XGBoost ou choisir un autre mod√®le.")
                                        import sys
                                        sys.exit()
                                
                                # M√©triques de validation avanc√©es
                                st.subheader("M√©triques de validation du mod√®le")
                                
                                # Calculer les erreurs
                                residuals = y_test - y_pred
                                abs_errors = np.abs(residuals)
                                
                                # 1. RMSE (Root Mean Squared Error)
                                rmse = np.sqrt(np.mean(residuals ** 2))
                                
                                # 2. MAE (Mean Absolute Error)
                                mae = np.mean(abs_errors)
                                
                                # 3. MAPE (Mean Absolute Percentage Error)
                                mape = np.mean(abs_errors / np.abs(y_test)) * 100
                                
                                # 4. R¬≤ (Coefficient of determination)
                                y_test_mean = np.mean(y_test)
                                ss_total = np.sum((y_test - y_test_mean) ** 2)
                                ss_residual = np.sum(residuals ** 2)
                                r2 = 1 - (ss_residual / ss_total) if ss_total != 0 else 0
                                
                                # 5. Adjusted R¬≤ (p√©nalise les mod√®les trop complexes)
                                n = len(y_test)
                                p = len(feature_vars)
                                adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1)) if n > p + 1 else 0
                                
                                # Afficher les m√©triques dans un tableau
                                metrics_df = pd.DataFrame({
                                    'M√©trique': ['RMSE', 'MAE', 'MAPE (%)', 'R¬≤', 'R¬≤ ajust√©'],
                                    'Valeur': [rmse, mae, mape, r2, adj_r2],
                                    'Description': [
                                        'Erreur quadratique moyenne (racine carr√©e)',
                                        'Erreur absolue moyenne',
                                        'Erreur absolue moyenne en pourcentage',
                                        'Coefficient de d√©termination',
                                        'R¬≤ ajust√© au nombre de variables'
                                    ]
                                })
                                
                                st.dataframe(metrics_df)
                                
                                # Afficher un r√©sum√© des m√©triques principales
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("RMSE", f"{rmse:.4f}")
                                with col2:
                                    st.metric("R¬≤", f"{r2:.4f}")
                                with col3:
                                    st.metric("MAPE (%)", f"{mape:.2f}")
                                
                                # Cr√©er un tableau pour comparer les pr√©dictions et les valeurs r√©elles
                                prediction_df = pd.DataFrame({
                                    'Valeur r√©elle': y_test,
                                    'Pr√©diction': y_pred,
                                    'Erreur': residuals,
                                    'Erreur relative (%)': (residuals / y_test) * 100
                                })
                                
                                st.subheader("Comparaison des pr√©dictions et valeurs r√©elles")
                                st.dataframe(prediction_df)
                                
                                # Analyse des r√©sidus
                                st.subheader("Analyse des r√©sidus")
                                
                                # Graphique des r√©sidus
                                fig_residuals = go.Figure()
                                
                                # Histogramme des r√©sidus
                                fig_residuals.add_trace(go.Histogram(
                                    x=residuals,
                                    name='Distribution des r√©sidus',
                                    opacity=0.7,
                                    marker_color='blue'
                                ))
                                
                                # Mise en forme du graphique
                                fig_residuals.update_layout(
                                    title='Distribution des r√©sidus',
                                    xaxis_title='R√©sidu',
                                    yaxis_title='Fr√©quence',
                                    showlegend=False
                                )
                                
                                st.plotly_chart(fig_residuals, use_container_width=True)
                                
                                
                                
                            except Exception as e:
                                st.error(f"Erreur lors de l'ajustement du mod√®le : {e}")
                                import traceback
                                st.code(traceback.format_exc())
                        else:
                            st.warning("Pas assez de donn√©es pour diviser en ensembles d'entra√Ænement et de test.")
                    else:
                        st.warning("Les donn√©es contiennent trop de valeurs manquantes pour ajuster un mod√®le.")
                else:
                    st.warning("Veuillez s√©lectionner au moins une variable explicative.")
        
            # Approche avec extraction de caract√©ristiques temporelles (FATS)
            elif model_type == "XGBoost avec FATS":
                st.markdown("""
                ## XGBoost avec extraction de caract√©ristiques temporelles (FATS)
                
                Cette approche extrait d'abord des caract√©ristiques statistiques significatives des s√©ries temporelles 
                (comme la moyenne, l'√©cart-type, la pente, etc.) pour chaque lot et param√®tre, puis utilise ces caract√©ristiques 
                pour construire un mod√®le XGBoost qui pr√©dit le param√®tre cible.
                """)
                
                # S√©lectionner les param√®tres pour l'extraction des caract√©ristiques
                feature_params = st.multiselect(
                    "Param√®tres pour l'extraction des caract√©ristiques",
                    options=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time', pred_param]],
                    default=[col for col in data.columns if col not in ['Batch name', 'Step', 'Time', pred_param]][:3],
                    key="fats_feature_params"
                )
                
                # Pr√©paration des donn√©es avec FATS
                if feature_params:
                    with st.spinner('Extraction des caract√©ristiques temporelles en cours...'):
                        # Utiliser toutes les donn√©es pour l'extraction des caract√©ristiques, pas seulement un lot
                        features_df = prepare_features_for_xgboost(data, pred_param, feature_params, pred_step if pred_step != "Toutes les √©tapes" else None)
                        
                        if len(features_df) > 3:  # V√©rifier qu'il y a assez de donn√©es
                            # Afficher les caract√©ristiques extraites
                            st.subheader("Aper√ßu des caract√©ristiques extraites des s√©ries temporelles")
                            st.dataframe(features_df.head())
                            
                            # Afficher le nombre total de caract√©ristiques
                            num_features = len(features_df.columns) - 2  # -2 pour 'Batch name' et 'target'
                            st.info(f"Nombre total de caract√©ristiques extraites: {num_features}")
                            
                            # Pr√©parer X et y
                            X = features_df.drop(['Batch name', 'target'], axis=1).values
                            y = features_df['target'].values
                            
                            # Division entra√Ænement/test comme avant
                            train_pct = st.slider(
                                "Pourcentage de donn√©es pour l'entra√Ænement",
                                min_value=50,
                                max_value=90,
                                value=70,
                                step=5,
                                key="fats_train_pct"
                            )
                            
                            split_idx = int(len(X) * train_pct / 100)
                            
                            # M√©langer les donn√©es pour √©viter les biais
                            indices = np.random.permutation(len(X))
                            X, y = X[indices], y[indices]
                            
                            X_train, X_test = X[:split_idx], X[split_idx:]
                            y_train, y_test = y[:split_idx], y[split_idx:]
                            
                            # Entra√Æner XGBoost
                            try:
                                import xgboost as xgb
                                
                                # Options pour XGBoost
                                n_estimators = st.slider("Nombre d'arbres (n_estimators)", 50, 300, 100, 50)
                                max_depth = st.slider("Profondeur maximale des arbres (max_depth)", 2, 10, 3, 1)
                                learning_rate = st.select_slider("Taux d'apprentissage (learning_rate)", 
                                                               options=[0.01, 0.05, 0.1, 0.2, 0.3], 
                                                               value=0.1)
                                
                                model = xgb.XGBRegressor(
                                    n_estimators=n_estimators,
                                    max_depth=max_depth,
                                    learning_rate=learning_rate
                                )
                                
                                with st.spinner('Entra√Ænement du mod√®le XGBoost en cours...'):
                                    model.fit(X_train, y_train)
                                
                                # Pr√©dictions et √©valuation
                                y_pred = model.predict(X_test)
                                
                                # Calculer les m√©triques d'√©valuation
                                residuals = y_test - y_pred
                                abs_errors = np.abs(residuals)
                                
                                rmse = np.sqrt(np.mean(residuals ** 2))
                                mae = np.mean(abs_errors)
                                mape = np.mean(abs_errors / np.abs(y_test)) * 100
                                
                                y_test_mean = np.mean(y_test)
                                ss_total = np.sum((y_test - y_test_mean) ** 2)
                                ss_residual = np.sum(residuals ** 2)
                                r2 = 1 - (ss_residual / ss_total) if ss_total != 0 else 0
                                
                                n = len(y_test)
                                p = num_features
                                adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1)) if n > p + 1 else 0
                                
                                # Afficher les m√©triques
                                metrics_df = pd.DataFrame({
                                    'M√©trique': ['RMSE', 'MAE', 'MAPE (%)', 'R¬≤', 'R¬≤ ajust√©'],
                                    'Valeur': [rmse, mae, mape, r2, adj_r2],
                                    'Description': [
                                        'Erreur quadratique moyenne (racine carr√©e)',
                                        'Erreur absolue moyenne',
                                        'Erreur absolue moyenne en pourcentage',
                                        'Coefficient de d√©termination',
                                        'R¬≤ ajust√© au nombre de variables'
                                    ]
                                })
                                
                                st.subheader("M√©triques de performance du mod√®le")
                                st.dataframe(metrics_df)
                                
                                # Afficher les m√©triques principales
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("RMSE", f"{rmse:.4f}")
                                with col2:
                                    st.metric("R¬≤", f"{r2:.4f}")
                                with col3:
                                    st.metric("MAPE (%)", f"{mape:.2f}")
                                
                                # Afficher l'importance des caract√©ristiques
                                feature_names = features_df.drop(['Batch name', 'target'], axis=1).columns
                                importance_df = pd.DataFrame({
                                    'Feature': feature_names,
                                    'Importance': model.feature_importances_
                                }).sort_values(by='Importance', ascending=False)
                                
                                st.subheader("Top 15 des caract√©ristiques les plus importantes")
                                
                                # Graphique d'importance des caract√©ristiques (top 15)
                                top_features = importance_df.head(15)
                                fig_importance = go.Figure()
                                fig_importance.add_trace(go.Bar(
                                    x=top_features['Importance'],
                                    y=top_features['Feature'],
                                    orientation='h',
                                    marker_color='blue'
                                ))
                                fig_importance.update_layout(
                                    title='Importance des caract√©ristiques temporelles',
                                    xaxis_title='Importance',
                                    yaxis_title='Caract√©ristique',
                                )
                                st.plotly_chart(fig_importance, use_container_width=True)
                                
                                # Table compl√®te d'importance des caract√©ristiques
                                with st.expander("Voir l'importance de toutes les caract√©ristiques"):
                                    st.dataframe(importance_df)
                                
                                # Comparaison des pr√©dictions et valeurs r√©elles
                                prediction_df = pd.DataFrame({
                                    'Valeur r√©elle': y_test,
                                    'Pr√©diction': y_pred,
                                    'Erreur': residuals,
                                    'Erreur relative (%)': (residuals / y_test) * 100
                                })
                                
                                st.subheader("Comparaison des pr√©dictions et valeurs r√©elles")
                                st.dataframe(prediction_df)
                                
                                # Graphique des pr√©dictions vs valeurs r√©elles
                                fig = go.Figure()
                                fig.add_trace(go.Scatter(
                                    x=y_test,
                                    y=y_pred,
                                    mode='markers',
                                    name='Test set',
                                    marker=dict(color='blue', size=8)
                                ))
                                
                                # Ligne de pr√©diction parfaite
                                min_val = min(min(y_test), min(y_pred))
                                max_val = max(max(y_test), max(y_pred))
                                fig.add_trace(go.Scatter(
                                    x=[min_val, max_val],
                                    y=[min_val, max_val],
                                    mode='lines',
                                    name='Pr√©diction parfaite',
                                    line=dict(color='red', dash='dash')
                                ))
                                
                                fig.update_layout(
                                    title=f'Pr√©dictions vs Valeurs R√©elles pour {pred_param}',
                                    xaxis_title='Valeur R√©elle',
                                    yaxis_title='Valeur Pr√©dite',
                                    legend_title='Donn√©es'
                                )
                                
                                st.plotly_chart(fig, use_container_width=True)
                                
                                # Analyse des r√©sidus
                                st.subheader("Analyse des r√©sidus")
                                
                                # Histogramme des r√©sidus
                                fig_residuals = go.Figure()
                                fig_residuals.add_trace(go.Histogram(
                                    x=residuals,
                                    opacity=0.7,
                                    marker_color='blue'
                                ))
                                fig_residuals.update_layout(
                                    title='Distribution des r√©sidus',
                                    xaxis_title='R√©sidu',
                                    yaxis_title='Fr√©quence',
                                )
                                st.plotly_chart(fig_residuals, use_container_width=True)
                                
                                
                                
                            except ImportError:
                                st.error("La biblioth√®que XGBoost n'est pas disponible. Veuillez installer XGBoost pour utiliser cette fonctionnalit√©.")
                            except Exception as e:
                                st.error(f"Erreur lors de l'ajustement du mod√®le : {e}")
                                import traceback
                                st.code(traceback.format_exc())
                        else:
                            st.warning("Pas assez de lots pour entra√Æner un mod√®le avec les caract√©ristiques extraites. Un minimum de 4 lots est n√©cessaire.")
                else:
                    st.warning("Veuillez s√©lectionner au moins un param√®tre pour l'extraction des caract√©ristiques.")
        else:
            st.warning("Pas de donn√©es disponibles pour le lot et l'√©tape s√©lectionn√©s.")
    
    
    

st.markdown("---")
st.markdown("**Application d√©velopp√©e pour Sanofi**")
st.write("**Version de Streamlit :**", st.__version__)
